{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md                datasets.py              \u001b[31mexample.py\u001b[m\u001b[m\r\n",
      "\u001b[34m__pycache__\u001b[m\u001b[m              \u001b[31mdownload_cornell.sh\u001b[m\u001b[m      seq2seq-lang-model.ipynb\r\n",
      "\u001b[34mdata\u001b[m\u001b[m                     \u001b[31mdownload_opensubs.sh\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\r\n",
      "\r\n",
      "mkdir -p data/cornell\r\n",
      "cd data/cornell\r\n",
      "wget https://github.com/Conchylicultor/DeepQA/raw/master/data/cornell/movie_conversations.txt\r\n",
      "wget https://github.com/Conchylicultor/DeepQA/raw/master/data/cornell/movie_lines.txt\r\n"
     ]
    }
   ],
   "source": [
    "! cat download_cornell.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-06-22 18:13:55--  https://github.com/Conchylicultor/DeepQA/raw/master/data/cornell/movie_conversations.txt\n",
      "Resolving github.com (github.com)... 140.82.118.4\n",
      "Connecting to github.com (github.com)|140.82.118.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/Conchylicultor/DeepQA/master/data/cornell/movie_conversations.txt [following]\n",
      "--2019-06-22 18:13:55--  https://raw.githubusercontent.com/Conchylicultor/DeepQA/master/data/cornell/movie_conversations.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6760930 (6.4M) [text/plain]\n",
      "Saving to: ‘movie_conversations.txt’\n",
      "\n",
      "movie_conversations 100%[===================>]   6.45M  1.87MB/s    in 3.4s    \n",
      "\n",
      "2019-06-22 18:13:59 (1.87 MB/s) - ‘movie_conversations.txt’ saved [6760930/6760930]\n",
      "\n",
      "--2019-06-22 18:13:59--  https://github.com/Conchylicultor/DeepQA/raw/master/data/cornell/movie_lines.txt\n",
      "Resolving github.com (github.com)... 140.82.118.4\n",
      "Connecting to github.com (github.com)|140.82.118.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/Conchylicultor/DeepQA/master/data/cornell/movie_lines.txt [following]\n",
      "--2019-06-22 18:14:00--  https://raw.githubusercontent.com/Conchylicultor/DeepQA/master/data/cornell/movie_lines.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 34641919 (33M) [text/plain]\n",
      "Saving to: ‘movie_lines.txt’\n",
      "\n",
      "movie_lines.txt     100%[===================>]  33.04M  4.74MB/s    in 11s     \n",
      "\n",
      "2019-06-22 18:14:12 (3.01 MB/s) - ‘movie_lines.txt’ saved [34641919/34641919]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! sh download_cornell.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mcornell\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "! ls data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import argparse\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'data/cornell'\n",
    "max_sentence_length = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.OpensubsDatarea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83097/83097 [00:05<00:00, 14642.59it/s]\n"
     ]
    }
   ],
   "source": [
    "data = datasets.readCornellData(dataset_path, max_len=max_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83097/83097 [00:08<00:00, 9914.08it/s] \n"
     ]
    }
   ],
   "source": [
    "data = [(preprocess_sentence(a), preprocess_sentence(b)) for (a,b) in create_dataset(num_examples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5256"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('there', 'where'),\n",
       " ('wow', 'lets go'),\n",
       " ('she okay', 'i hope so'),\n",
       " ('who', 'joey'),\n",
       " ('its more', 'expensive'),\n",
       " ('thirtytwo', 'get out'),\n",
       " ('okay', 'im fine im'),\n",
       " ('sure i do', 'why'),\n",
       " ('who', 'dorsey'),\n",
       " ('dorsey', 'i hate him')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    \n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\" \n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    \n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "    \n",
    "    w = w.rstrip().strip()\n",
    "    \n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class creates a word -> index mapping (e.g,. \"dad\" -> 5) and vice-versa \n",
    "# (e.g., 5 -> \"dad\") for each language,\n",
    "\n",
    "def create_index(phrases):   \n",
    "    \n",
    "    word2idx = {}\n",
    "    idx2word = {}\n",
    "    vocab = set()\n",
    "    \n",
    "    for phrase in phrases:\n",
    "        vocab.update(phrase.split(' '))\n",
    "    \n",
    "    vocab = sorted(vocab)\n",
    "    \n",
    "    word2idx['<pad>'] = 0\n",
    "    for index, word in enumerate(vocab):\n",
    "        word2idx[word] = index + 1\n",
    "    \n",
    "    for word, index in word2idx.items():\n",
    "        idx2word[index] = word\n",
    "        \n",
    "    return word2idx, idx2word, vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "# Import TensorFlow >= 1.10 and enable eager execution\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(num_examples, dataset_name = 'cornell', max_sentence_length= 10):\n",
    "    dataset_path = 'data/{}'.format(dataset_name)\n",
    "\n",
    "    if dataset_name == \"cornell\":\n",
    "        data = datasets.readCornellData(dataset_path, max_len=max_sentence_length)\n",
    "    elif dataset_name == \"opensubs\":\n",
    "        data = datasets.readOpensubsData(dataset_path, max_len=max_sentence_length)\n",
    "    else:\n",
    "        raise ValueError(\"Unrecognized dataset: {!r}\".format(dataset_name))\n",
    "    \n",
    "    return data\n",
    "\n",
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "def preprocess_sentence(s):\n",
    "    return '<start> ' + s + ' <end>'\n",
    "\n",
    "\n",
    "def load_dataset(num_examples):\n",
    "    # creating cleaned input, output pairs\n",
    "    pairs = [(preprocess_sentence(a), preprocess_sentence(b)) for (a,b) in create_dataset(num_examples)]\n",
    "\n",
    "    # index language using the class defined above    \n",
    "    word2idx, idx2word, vocab = create_index([p for ps in pairs for p in ps])\n",
    "    # Vectorize the input and target languages\n",
    "    \n",
    "    # question sentences\n",
    "    input_tensor = [[word2idx[w] for w in qs.split(' ')] for qs, a in pairs]\n",
    "    \n",
    "    # answer sentences\n",
    "    target_tensor = [[word2idx[w] for w in a.split(' ')] for q, a in pairs]\n",
    "    \n",
    "    # Calculate max_length of input and output tensor\n",
    "    # Here, we'll set those to the longest sentence in the dataset\n",
    "    max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
    "    \n",
    "    # Padding the input and output tensor to the maximum length\n",
    "    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
    "                                                                 maxlen=max_length_inp,\n",
    "                                                                 padding='post')\n",
    "    \n",
    "    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
    "                                                                  maxlen=max_length_tar, \n",
    "                                                                  padding='post')\n",
    "    \n",
    "    return input_tensor, target_tensor, (word2idx, idx2word, vocab), max_length_inp, max_length_tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83097/83097 [00:05<00:00, 14890.06it/s]\n"
     ]
    }
   ],
   "source": [
    "# Try experimenting with the size of that dataset\n",
    "num_examples = 30000\n",
    "input_tensor, target_tensor, dict_index, max_length_inp, max_length_targ = load_dataset(num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4204, 4204, 1052, 1052)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TF dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_size = len(dict_index[0])\n",
    "vocab_tar_size = vocab_size\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru(units):\n",
    "  # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\n",
    "  # the code automatically does that.\n",
    "    if tf.test.is_gpu_available():\n",
    "        return tf.keras.layers.CuDNNGRU(units, \n",
    "                                    return_sequences=True, \n",
    "                                    return_state=True, \n",
    "                                    recurrent_initializer='glorot_uniform')\n",
    "    else:\n",
    "        return tf.keras.layers.GRU(units, \n",
    "                               return_sequences=True, \n",
    "                               return_state=True, \n",
    "                               recurrent_activation='sigmoid', \n",
    "                               recurrent_initializer='glorot_uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.enc_units)\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)        \n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.dec_units)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # used for attention\n",
    "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        \n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        \n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n",
    "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
    "        \n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output shape == (batch_size * 1, vocab)\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state, attention_weights\n",
    "        \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.dec_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = 1 - np.equal(real, 0)\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_size, embedding_dim, units, BATCH_SIZE)\n",
    "optimizer = tf.train.AdamOptimizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_index[0]['<start>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = dict_index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training explained\n",
    "\n",
    "1. Pass the *input* through the *encoder* which return *encoder output* and the *encoder hidden state*.\n",
    "2. The encoder output, encoder hidden state and the decoder input (which is the *start token*) is passed to the decoder.\n",
    "3. The decoder returns the *predictions* and the *decoder hidden state*.\n",
    "4. The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n",
    "5. Use *teacher forcing* to decide the next input to the decoder.\n",
    "6. *Teacher forcing* is the technique where the *target word* is passed as the *next input* to the decoder.\n",
    "7. The final step is to calculate the gradients and apply it to the optimizer and backpropagate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 1.4769\n",
      "Epoch 1 Loss 1.5263\n",
      "Time taken for 1 epoch 221.04495096206665 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.3829\n",
      "Epoch 2 Loss 1.4485\n",
      "Time taken for 1 epoch 211.28798985481262 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.5323\n",
      "Epoch 3 Loss 1.3965\n",
      "Time taken for 1 epoch 211.8134160041809 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.2901\n",
      "Epoch 4 Loss 1.3429\n",
      "Time taken for 1 epoch 218.80016374588013 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.2519\n",
      "Epoch 5 Loss 1.2952\n",
      "Time taken for 1 epoch 227.82110285758972 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(dataset):\n",
    "        loss = 0\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output, enc_hidden = encoder(inp, hidden)\n",
    "            \n",
    "            dec_hidden = enc_hidden\n",
    "            \n",
    "            dec_input = tf.expand_dims([word2idx['<start>']] * BATCH_SIZE, 1)       \n",
    "            \n",
    "            # Teacher forcing - feeding the target as the next input\n",
    "            for t in range(1, targ.shape[1]):\n",
    "                # passing enc_output to the decoder\n",
    "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "                \n",
    "                loss += loss_function(targ[:, t], predictions)\n",
    "                \n",
    "                # using teacher forcing\n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "        \n",
    "        batch_loss = (loss / int(targ.shape[1]))\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        variables = encoder.variables + decoder.variables\n",
    "        \n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / N_BATCH))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction \n",
    "* The evaluate function is similar to the training loop, except we don't use *teacher forcing* here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n",
    "* Stop predicting when the model predicts the *end token*.\n",
    "* And store the *attention weights for every time step*.\n",
    "\n",
    "Note: The encoder output is calculated only once for one input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, encoder, decoder, dictionary_index,  max_length):\n",
    "    word2idx = dictionary_index[0]\n",
    "    idx2word = dictionary_index[1]\n",
    "    attention_plot = np.zeros((max_length, max_length))\n",
    "    \n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    inputs = [word2idx[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([word2idx['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        \n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += idx2word[predicted_id] + ' '\n",
    "\n",
    "        if idx2word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "        \n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    \n",
    "    fontdict = {'fontsize': 14}\n",
    "    \n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(sentence, encoder, decoder, dictionary_index, max_length):\n",
    "    result, sentence, attention_plot = evaluate(sentence, encoder, decoder, dictionary_index, max_length)\n",
    "        \n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    \n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> hi <end>\n",
      "Predicted translation: yes <end> \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer('hi', encoder, decoder, dict_index, max_length_targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> what do you like to do <end>\n",
      "Predicted translation: yeah <end> \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer('what do you like to do', encoder, decoder, dict_index, max_length_targ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratchpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 6)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length_inp, max_length_targ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  36, 2430, 1597,   35,    0,    0], dtype=int32),\n",
       " array([1, 1, 1, 1, 0, 0]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tensor_train[0], 1 - np.equal(target_tensor_train[0], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  36, 1580,   35,    0,    0], dtype=int32),\n",
       " array([  36, 2430, 1597,   35,    0,    0], dtype=int32))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor_train[0], target_tensor_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1861"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_index = create_index([q for q, a in data])\n",
    "q_word2idx = q_index[0]\n",
    "len(q_word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1618],\n",
       " [1823],\n",
       " [1471, 1202],\n",
       " [1781],\n",
       " [813, 1097],\n",
       " [1626],\n",
       " [1202],\n",
       " [1576, 776, 449],\n",
       " [1781],\n",
       " [465],\n",
       " [449, 1770],\n",
       " [1781],\n",
       " [1845, 1353, 1521],\n",
       " [1784],\n",
       " [546],\n",
       " [1613, 479],\n",
       " [1162],\n",
       " [1845, 1622],\n",
       " [1427, 811],\n",
       " [1162],\n",
       " [1629],\n",
       " [1162, 1162, 1753],\n",
       " [1316],\n",
       " [76],\n",
       " [76],\n",
       " [76],\n",
       " [76],\n",
       " [723, 749],\n",
       " [1612, 648],\n",
       " [760, 1745, 811],\n",
       " [1358],\n",
       " [682],\n",
       " [1128],\n",
       " [1834],\n",
       " [819],\n",
       " [713],\n",
       " [726, 813, 1035],\n",
       " [723],\n",
       " [1767, 1425],\n",
       " [648, 973],\n",
       " [15, 13],\n",
       " [1483],\n",
       " [348],\n",
       " [1779, 1207],\n",
       " [105],\n",
       " [1684],\n",
       " [934, 969],\n",
       " [1602, 1035],\n",
       " [776, 969, 1845],\n",
       " [713],\n",
       " [724, 1702],\n",
       " [1522],\n",
       " [1715, 612],\n",
       " [246, 384],\n",
       " [1787, 821],\n",
       " [1303, 1808],\n",
       " [1727],\n",
       " [1358],\n",
       " [1619, 1406],\n",
       " [1405, 1206],\n",
       " [1834],\n",
       " [449, 1770],\n",
       " [1770, 1617],\n",
       " [813, 1570],\n",
       " [1162],\n",
       " [1840],\n",
       " [1202],\n",
       " [648, 1781],\n",
       " [1845, 1202],\n",
       " [1845, 1202],\n",
       " [588, 811],\n",
       " [863, 449, 811],\n",
       " [1834],\n",
       " [154],\n",
       " [1388],\n",
       " [1781],\n",
       " [1678],\n",
       " [1087],\n",
       " [67],\n",
       " [385],\n",
       " [653, 811],\n",
       " [1160],\n",
       " [590],\n",
       " [843],\n",
       " [1207],\n",
       " [1474],\n",
       " [1235],\n",
       " [484],\n",
       " [500],\n",
       " [500],\n",
       " [1459],\n",
       " [1178],\n",
       " [1848, 1576],\n",
       " [500],\n",
       " [1599],\n",
       " [726],\n",
       " [1387],\n",
       " [1387],\n",
       " [1845, 1202],\n",
       " [791, 1618],\n",
       " [719],\n",
       " [1176],\n",
       " [1483],\n",
       " [1547],\n",
       " [1105],\n",
       " [786, 641],\n",
       " [1502],\n",
       " [1424, 811],\n",
       " [954],\n",
       " [1777],\n",
       " [791, 25, 691],\n",
       " [1772, 1611],\n",
       " [25, 228],\n",
       " [1178],\n",
       " [1840],\n",
       " [1770],\n",
       " [1639],\n",
       " [491],\n",
       " [1840],\n",
       " [1162],\n",
       " [1222],\n",
       " [1649],\n",
       " [73],\n",
       " [167, 267],\n",
       " [174],\n",
       " [723],\n",
       " [174],\n",
       " [1704],\n",
       " [1840, 793],\n",
       " [1330],\n",
       " [1840],\n",
       " [1840],\n",
       " [1162],\n",
       " [1193, 1840],\n",
       " [1613, 573],\n",
       " [776, 1622, 1521],\n",
       " [1777],\n",
       " [1770],\n",
       " [1107, 873],\n",
       " [323],\n",
       " [293],\n",
       " [164],\n",
       " [66],\n",
       " [1189, 356],\n",
       " [246],\n",
       " [1162],\n",
       " [1162, 1770],\n",
       " [1162],\n",
       " [1162],\n",
       " [1162, 1770],\n",
       " [1840],\n",
       " [819],\n",
       " [1202],\n",
       " [1375],\n",
       " [1834],\n",
       " [819],\n",
       " [409],\n",
       " [726, 819],\n",
       " [1162],\n",
       " [1840, 1576],\n",
       " [703, 698],\n",
       " [291],\n",
       " [1763],\n",
       " [1178],\n",
       " [689, 1178],\n",
       " [1840],\n",
       " [711],\n",
       " [1840, 1502],\n",
       " [808, 811, 1404],\n",
       " [1582],\n",
       " [391],\n",
       " [1770, 1613],\n",
       " [1079],\n",
       " [1755, 414],\n",
       " [1767, 478],\n",
       " [984, 1294],\n",
       " [738],\n",
       " [461, 1582],\n",
       " [1251],\n",
       " [1521, 722, 1810],\n",
       " [1329],\n",
       " [497],\n",
       " [1258],\n",
       " [1840],\n",
       " [1774],\n",
       " [1770, 808, 811],\n",
       " [1258],\n",
       " [588, 811],\n",
       " [1533],\n",
       " [1792],\n",
       " [1781],\n",
       " [329],\n",
       " [1845, 1576],\n",
       " [1176],\n",
       " [719, 1754, 641],\n",
       " [616],\n",
       " [1062],\n",
       " [703, 895],\n",
       " [1840],\n",
       " [1840],\n",
       " [167, 267],\n",
       " [1840],\n",
       " [1840],\n",
       " [167, 267],\n",
       " [272],\n",
       " [1840, 160],\n",
       " [167, 267],\n",
       " [1840, 160],\n",
       " [788, 1533],\n",
       " [887, 1035],\n",
       " [1119, 1528],\n",
       " [1445, 161],\n",
       " [722, 119],\n",
       " [215],\n",
       " [1359],\n",
       " [891],\n",
       " [194],\n",
       " [416],\n",
       " [969],\n",
       " [584],\n",
       " [1134],\n",
       " [1313],\n",
       " [1449],\n",
       " [969],\n",
       " [752],\n",
       " [969],\n",
       " [556],\n",
       " [969],\n",
       " [1162, 1609],\n",
       " [1556],\n",
       " [1220],\n",
       " [1845, 576],\n",
       " [663, 264],\n",
       " [1612, 811],\n",
       " [1373],\n",
       " [1772, 1627],\n",
       " [1781],\n",
       " [417],\n",
       " [808, 703],\n",
       " [723],\n",
       " [776, 1426, 811],\n",
       " [225],\n",
       " [835],\n",
       " [726],\n",
       " [575],\n",
       " [798],\n",
       " [422],\n",
       " [1037],\n",
       " [1003],\n",
       " [1002],\n",
       " [25, 628],\n",
       " [703, 788, 1091],\n",
       " [1162],\n",
       " [113],\n",
       " [25, 964],\n",
       " [1843, 1770],\n",
       " [92],\n",
       " [1245],\n",
       " [1375],\n",
       " [1112],\n",
       " [1032],\n",
       " [1032, 1770],\n",
       " [1840],\n",
       " [1104, 729],\n",
       " [1834],\n",
       " [590],\n",
       " [1733],\n",
       " [1781, 927],\n",
       " [198],\n",
       " [786],\n",
       " [1202],\n",
       " [1358],\n",
       " [384],\n",
       " [384],\n",
       " [1613, 1379],\n",
       " [934, 1627],\n",
       " [1845, 1427],\n",
       " [1193, 1193],\n",
       " [1080],\n",
       " [1087],\n",
       " [1840, 776, 893],\n",
       " [648],\n",
       " [1193, 1176],\n",
       " [1588],\n",
       " [600],\n",
       " [1770],\n",
       " [537, 1035],\n",
       " [1771],\n",
       " [788, 1533],\n",
       " [95],\n",
       " [726],\n",
       " [745, 1483],\n",
       " [182],\n",
       " [1770, 808, 811],\n",
       " [1135],\n",
       " [723],\n",
       " [745, 1483],\n",
       " [1772, 1627],\n",
       " [1152],\n",
       " [1553, 811],\n",
       " [788, 1533],\n",
       " [1767, 791],\n",
       " [788, 89],\n",
       " [1777],\n",
       " [934, 1770],\n",
       " [1326],\n",
       " [1326],\n",
       " [831],\n",
       " [1834],\n",
       " [414],\n",
       " [1609],\n",
       " [194],\n",
       " [461, 1104],\n",
       " [1189, 356],\n",
       " [171],\n",
       " [726],\n",
       " [223],\n",
       " [335, 1777],\n",
       " [1162, 788, 963],\n",
       " [1845, 461],\n",
       " [1733, 1733],\n",
       " [1770, 808, 811],\n",
       " [813, 1271],\n",
       " [1162],\n",
       " [1733, 1733],\n",
       " [25, 1793],\n",
       " [1840],\n",
       " [811, 452],\n",
       " [1189, 356],\n",
       " [1840],\n",
       " [40],\n",
       " [1770],\n",
       " [1618],\n",
       " [1689, 40],\n",
       " [1840, 811, 808],\n",
       " [1770],\n",
       " [25, 265],\n",
       " [1840, 1502],\n",
       " [251, 1662],\n",
       " [1189, 1770],\n",
       " [713],\n",
       " [501, 1528],\n",
       " [1162],\n",
       " [1834],\n",
       " [648, 642],\n",
       " [1613, 955],\n",
       " [1642, 641, 1702],\n",
       " [1193, 1162],\n",
       " [258],\n",
       " [1751, 1627],\n",
       " [726, 749],\n",
       " [258],\n",
       " [1162],\n",
       " [1754, 449],\n",
       " [1840],\n",
       " [1642, 173],\n",
       " [1193, 1162],\n",
       " [40],\n",
       " [1777],\n",
       " [834],\n",
       " [1763],\n",
       " [457],\n",
       " [1162],\n",
       " [959],\n",
       " [1162],\n",
       " [335, 791],\n",
       " [1193, 1162],\n",
       " [641, 1777],\n",
       " [726, 384],\n",
       " [76],\n",
       " [1202, 1202],\n",
       " [1202],\n",
       " [641, 56],\n",
       " [1417],\n",
       " [1162, 984, 1162],\n",
       " [606],\n",
       " [1035, 1702, 1613],\n",
       " [723, 1117],\n",
       " [1142],\n",
       " [25, 191],\n",
       " [1193, 1119, 642],\n",
       " [713],\n",
       " [713],\n",
       " [1840],\n",
       " [1845, 559],\n",
       " [760, 1111],\n",
       " [1840, 776, 449],\n",
       " [1387],\n",
       " [703, 437],\n",
       " [1162, 788, 1174],\n",
       " [1840, 703, 437],\n",
       " [613, 1001],\n",
       " [1770],\n",
       " [1770, 1770],\n",
       " [1840, 776, 437],\n",
       " [1840, 776, 449],\n",
       " [1429],\n",
       " [742, 1206],\n",
       " [1840],\n",
       " [1107, 716],\n",
       " [335, 1206, 791],\n",
       " [813, 954],\n",
       " [1502],\n",
       " [340],\n",
       " [1845, 475],\n",
       " [1840],\n",
       " [813, 1035],\n",
       " [1193, 1483],\n",
       " [1492, 1035],\n",
       " [1107, 1145],\n",
       " [1846, 1104],\n",
       " [1617, 1774],\n",
       " [1228],\n",
       " [776, 1812],\n",
       " [650],\n",
       " [650],\n",
       " [787, 776],\n",
       " [1412],\n",
       " [1770, 587],\n",
       " [1119, 1192],\n",
       " [213],\n",
       " [1590, 811],\n",
       " [1642, 1797],\n",
       " [242],\n",
       " [726],\n",
       " [1612, 811],\n",
       " [722, 647],\n",
       " [1684],\n",
       " [729],\n",
       " [1770],\n",
       " [1228],\n",
       " [498],\n",
       " [1781, 1745, 703],\n",
       " [788, 1533],\n",
       " [808, 811, 1840],\n",
       " [1840],\n",
       " [449, 1845],\n",
       " [1781],\n",
       " [701, 776],\n",
       " [632],\n",
       " [462],\n",
       " [595],\n",
       " [1193, 1834],\n",
       " [1104, 811],\n",
       " [1231, 1618],\n",
       " [1845, 438],\n",
       " [1774],\n",
       " [178],\n",
       " [1772, 1627],\n",
       " [1164],\n",
       " [1612, 1188],\n",
       " [654, 641],\n",
       " [1162, 1322],\n",
       " [1533],\n",
       " [837],\n",
       " [1193, 726, 1087],\n",
       " [1612, 811],\n",
       " [723],\n",
       " [460],\n",
       " [92, 1770],\n",
       " [723],\n",
       " [1162, 593],\n",
       " [1396],\n",
       " [1109, 918],\n",
       " [726],\n",
       " [726],\n",
       " [1202],\n",
       " [1834],\n",
       " [388],\n",
       " [1770],\n",
       " [1684, 639],\n",
       " [723, 1845],\n",
       " [1845, 1819],\n",
       " [1207, 1218, 1676],\n",
       " [1840],\n",
       " [1770, 808, 811],\n",
       " [461],\n",
       " [1854],\n",
       " [1855],\n",
       " [713],\n",
       " [1119, 1128, 808],\n",
       " [1119, 746],\n",
       " [1792],\n",
       " [1358],\n",
       " [1440],\n",
       " [53],\n",
       " [966],\n",
       " [27, 1348],\n",
       " [246, 1770],\n",
       " [246],\n",
       " [1493, 1702],\n",
       " [813, 993],\n",
       " [1398],\n",
       " [251, 920],\n",
       " [251, 1533],\n",
       " [27],\n",
       " [1466, 772],\n",
       " [715],\n",
       " [882, 882],\n",
       " [788, 1533],\n",
       " [1792],\n",
       " [813, 1202],\n",
       " [1834],\n",
       " [1774],\n",
       " [1844, 740, 740],\n",
       " [1097],\n",
       " [726, 1618],\n",
       " [251, 847],\n",
       " [1834],\n",
       " [1774],\n",
       " [726],\n",
       " [726],\n",
       " [210],\n",
       " [1193],\n",
       " [1772, 1627],\n",
       " [1021],\n",
       " [319],\n",
       " [776, 461],\n",
       " [745, 1483],\n",
       " [1770],\n",
       " [1770],\n",
       " [70],\n",
       " [1509],\n",
       " [1274],\n",
       " [1840, 1502],\n",
       " [1759],\n",
       " [1770, 808, 811],\n",
       " [703, 438],\n",
       " [1848, 755],\n",
       " [1564, 811],\n",
       " [1779, 1745],\n",
       " [490, 1090],\n",
       " [726, 221],\n",
       " [723, 1349],\n",
       " [1303, 675],\n",
       " [71, 1375],\n",
       " [1126],\n",
       " [1792],\n",
       " [1290],\n",
       " [1430],\n",
       " [1195],\n",
       " [1770],\n",
       " [335, 1206],\n",
       " [722, 414],\n",
       " [1770],\n",
       " [611],\n",
       " [590, 463],\n",
       " [1676],\n",
       " [1217],\n",
       " [358],\n",
       " [760, 1004],\n",
       " [743, 590],\n",
       " [1029],\n",
       " [1461],\n",
       " [1160],\n",
       " [1461],\n",
       " [496, 303],\n",
       " [715, 1041],\n",
       " [1193, 841],\n",
       " [742, 811],\n",
       " [990],\n",
       " [1781],\n",
       " [1840],\n",
       " [1805, 1035],\n",
       " [224],\n",
       " [47],\n",
       " [564],\n",
       " [696],\n",
       " [1770],\n",
       " [591],\n",
       " [1777, 1642],\n",
       " [1483],\n",
       " [1770],\n",
       " [537, 1035],\n",
       " [1770],\n",
       " [1323],\n",
       " [1193, 1119, 642],\n",
       " [567],\n",
       " [819],\n",
       " [1009],\n",
       " [776, 679, 1642],\n",
       " [776, 969, 1845],\n",
       " [1083],\n",
       " [1083],\n",
       " [179, 1233],\n",
       " [776, 653, 1642],\n",
       " [1787, 1627],\n",
       " [791, 708],\n",
       " [1475, 414],\n",
       " [1781, 509],\n",
       " [437, 703],\n",
       " [776, 1325],\n",
       " [776, 1325],\n",
       " [788, 1533],\n",
       " [1451, 1829],\n",
       " [1372],\n",
       " [349],\n",
       " [1208],\n",
       " [343],\n",
       " [449, 1845],\n",
       " [317],\n",
       " [1770, 1770],\n",
       " [1702],\n",
       " [1754, 1315],\n",
       " [1498],\n",
       " [776, 1427],\n",
       " [1772, 1611],\n",
       " [696],\n",
       " [1207, 1097],\n",
       " [1027],\n",
       " [1770],\n",
       " [1027],\n",
       " [1387],\n",
       " [1387],\n",
       " [1519],\n",
       " [1770],\n",
       " [625, 1702],\n",
       " [1848, 909],\n",
       " [723],\n",
       " [813, 612],\n",
       " [1781],\n",
       " [1781],\n",
       " [637, 1702],\n",
       " [1840],\n",
       " [246, 776, 262],\n",
       " [1763, 1840],\n",
       " [726, 855],\n",
       " [726, 517],\n",
       " [1770, 1792],\n",
       " [855],\n",
       " [641, 56],\n",
       " [1770, 1635],\n",
       " [1476],\n",
       " [1358],\n",
       " [1684, 713],\n",
       " [726, 813, 1035],\n",
       " [210],\n",
       " [1464],\n",
       " [648, 973],\n",
       " [776, 1812],\n",
       " [648, 973],\n",
       " [788, 1350],\n",
       " [1356],\n",
       " [92, 1331],\n",
       " [649],\n",
       " [1781],\n",
       " [1082],\n",
       " [788, 1457],\n",
       " [665],\n",
       " [713],\n",
       " [1162],\n",
       " [449, 1845],\n",
       " [1840],\n",
       " [776, 449],\n",
       " [1770, 808, 811],\n",
       " [1611, 1808],\n",
       " [500],\n",
       " [625, 1231],\n",
       " [246, 500],\n",
       " [177],\n",
       " [1840],\n",
       " [177],\n",
       " [176],\n",
       " [1840, 776, 80],\n",
       " [177],\n",
       " [1193, 1162],\n",
       " [1845, 461],\n",
       " [1162],\n",
       " [1608, 1845],\n",
       " [474],\n",
       " [1162],\n",
       " [1193, 1119, 642],\n",
       " [1840],\n",
       " [177],\n",
       " [1608, 1845],\n",
       " [177],\n",
       " [1178],\n",
       " [177],\n",
       " [1613, 1181],\n",
       " [1763],\n",
       " [177],\n",
       " [1770],\n",
       " [1193, 1119, 642],\n",
       " [1193],\n",
       " [1840, 776, 449],\n",
       " [1770, 1207],\n",
       " [1618],\n",
       " [722, 119],\n",
       " [1375],\n",
       " [1162],\n",
       " [1840],\n",
       " [121, 332],\n",
       " [115],\n",
       " [1162],\n",
       " [1162],\n",
       " [788, 863],\n",
       " [1820],\n",
       " [1763],\n",
       " [1642, 167, 1770],\n",
       " [1193, 1162],\n",
       " [776, 1794],\n",
       " [246, 1792],\n",
       " [1174, 1178],\n",
       " [1162],\n",
       " [814, 1339],\n",
       " [1611],\n",
       " [1845],\n",
       " [1193, 1817],\n",
       " [110, 1845],\n",
       " [1770],\n",
       " [1189, 356],\n",
       " [645],\n",
       " [1193],\n",
       " [1840, 1638],\n",
       " [1618],\n",
       " [776, 893],\n",
       " [776, 262],\n",
       " [1634, 1617],\n",
       " [1162, 451],\n",
       " [1600],\n",
       " [1098],\n",
       " [1098],\n",
       " [1638],\n",
       " [1098],\n",
       " [1845],\n",
       " [1845],\n",
       " [1098],\n",
       " [1840],\n",
       " [1193, 1162],\n",
       " [45],\n",
       " [1119, 1528],\n",
       " [1763, 1684, 1162],\n",
       " [1400],\n",
       " [1076],\n",
       " [703, 1368],\n",
       " [1777],\n",
       " [1400, 442],\n",
       " [430],\n",
       " [1162, 1483],\n",
       " [1174, 1035],\n",
       " [182],\n",
       " [1733],\n",
       " [1325],\n",
       " [1089],\n",
       " [1035],\n",
       " [1845],\n",
       " [587, 1770],\n",
       " [713],\n",
       " [713],\n",
       " [516],\n",
       " [392],\n",
       " [384],\n",
       " [1781, 1745, 811],\n",
       " [1087],\n",
       " [1772, 1627],\n",
       " [444],\n",
       " [1162],\n",
       " [1613, 451],\n",
       " [1162],\n",
       " [1215],\n",
       " [1162, 1162],\n",
       " [625, 1702],\n",
       " [414],\n",
       " [1772, 1627],\n",
       " [449, 811],\n",
       " [1834],\n",
       " [1754, 1191],\n",
       " [1162],\n",
       " [25, 601],\n",
       " [1772, 1641],\n",
       " [1023],\n",
       " [1162],\n",
       " [1837],\n",
       " [1162],\n",
       " [71, 1375],\n",
       " [694],\n",
       " [959],\n",
       " [1202],\n",
       " [1189, 356],\n",
       " [1804, 642],\n",
       " [1025],\n",
       " [1840],\n",
       " [1193, 694],\n",
       " [694],\n",
       " [1840],\n",
       " [1248],\n",
       " [859],\n",
       " [1608, 1845],\n",
       " [1120],\n",
       " [1840],\n",
       " [1010],\n",
       " [328],\n",
       " [1566],\n",
       " [734, 1035],\n",
       " [859],\n",
       " [1303, 142],\n",
       " [859],\n",
       " [1526],\n",
       " [788, 774],\n",
       " [910],\n",
       " [907],\n",
       " [1763],\n",
       " [1483],\n",
       " [859],\n",
       " [1781, 1745, 811],\n",
       " [764],\n",
       " [863, 1524],\n",
       " [1289, 1524],\n",
       " [1787, 1618],\n",
       " [184, 759],\n",
       " [1845, 934],\n",
       " [1577],\n",
       " [109, 1845, 1201],\n",
       " [569],\n",
       " [722, 647],\n",
       " [1840],\n",
       " [1193, 1162],\n",
       " [251],\n",
       " [69],\n",
       " [1772, 1148],\n",
       " [1292, 811],\n",
       " [1119, 152],\n",
       " [1770],\n",
       " [723, 157],\n",
       " [1193, 726],\n",
       " [1380],\n",
       " [1483],\n",
       " [726],\n",
       " [1770],\n",
       " [713],\n",
       " [1162, 246],\n",
       " [776, 1036, 811],\n",
       " [1834],\n",
       " [788, 1719],\n",
       " [315],\n",
       " [1321],\n",
       " [1792, 1174],\n",
       " [625, 1231],\n",
       " [550, 132],\n",
       " [1847, 1128],\n",
       " [1401],\n",
       " [1583],\n",
       " [72, 1035],\n",
       " [728],\n",
       " [1840],\n",
       " [237, 1190],\n",
       " [1523],\n",
       " [1523],\n",
       " [197],\n",
       " [1781, 1804],\n",
       " [1770],\n",
       " [1609],\n",
       " [1107, 1796],\n",
       " [130, 606],\n",
       " [599],\n",
       " [935],\n",
       " [1162],\n",
       " [1715, 1763],\n",
       " [1324],\n",
       " [458],\n",
       " [1845, 449],\n",
       " [1840],\n",
       " [580],\n",
       " [1733],\n",
       " [1102],\n",
       " [1839],\n",
       " [1286],\n",
       " [1202],\n",
       " [379],\n",
       " [1770, 808],\n",
       " [587, 1781],\n",
       " [726],\n",
       " [1666, 1035],\n",
       " [1487],\n",
       " [256, 811],\n",
       " [1770, 243],\n",
       " [776, 474],\n",
       " [1792],\n",
       " [915, 1178],\n",
       " [1804, 1616],\n",
       " [766],\n",
       " [1723],\n",
       " [164],\n",
       " [1617, 1770],\n",
       " [1770],\n",
       " [1840, 776, 80],\n",
       " [436],\n",
       " [436],\n",
       " [1162, 538],\n",
       " [1011],\n",
       " [1119, 1393],\n",
       " [384],\n",
       " [1763, 1684],\n",
       " [384],\n",
       " [1770],\n",
       " [384],\n",
       " [1770],\n",
       " [1564],\n",
       " [1770],\n",
       " [384],\n",
       " [1356],\n",
       " [246, 794],\n",
       " [470, 854],\n",
       " [25, 1770],\n",
       " [776, 262],\n",
       " [776, 625, 811],\n",
       " [1781, 1745, 811],\n",
       " [1035],\n",
       " [1781],\n",
       " [62],\n",
       " [1840],\n",
       " [1834],\n",
       " [1781, 80, 776],\n",
       " [1572, 1616],\n",
       " [1781, 437],\n",
       " [647, 1777],\n",
       " [1178, 1770],\n",
       " [1613, 168],\n",
       " [745, 1483],\n",
       " [1587],\n",
       " [1178, 1770],\n",
       " [1613, 1416],\n",
       " [491, 811],\n",
       " [1792, 1174],\n",
       " [171],\n",
       " [1770, 1745],\n",
       " [1733],\n",
       " [1655, 717],\n",
       " [1223],\n",
       " [585, 717],\n",
       " [1206, 1770],\n",
       " [474, 811],\n",
       " [813, 729],\n",
       " [1162],\n",
       " [1193, 776, 262],\n",
       " [1375],\n",
       " [1609],\n",
       " [1763],\n",
       " [1763, 1770],\n",
       " [1767, 719],\n",
       " [846],\n",
       " [1572, 1781],\n",
       " [1176],\n",
       " [195],\n",
       " [1792],\n",
       " [261, 776, 715],\n",
       " [703, 438],\n",
       " [1162],\n",
       " [1781, 437],\n",
       " [1770, 808, 811],\n",
       " [750, 897],\n",
       " [1792, 1174],\n",
       " [760],\n",
       " [1733],\n",
       " [1840],\n",
       " [726, 749],\n",
       " [726, 385],\n",
       " [1087],\n",
       " [1576, 1777],\n",
       " [1092],\n",
       " [417],\n",
       " [1537, 1423],\n",
       " [16, 1067],\n",
       " [1834],\n",
       " [1162, 1483],\n",
       " [970],\n",
       " [437, 1817],\n",
       " [1162],\n",
       " [1119, 642],\n",
       " [713],\n",
       " [713],\n",
       " [1115],\n",
       " [713, 1369],\n",
       " [1787, 1611],\n",
       " [936, 1702],\n",
       " [533, 1756],\n",
       " [678, 678],\n",
       " [1107, 1370],\n",
       " [1608, 1845],\n",
       " [1824],\n",
       " [1824],\n",
       " [1198],\n",
       " [1840, 1502],\n",
       " [1107, 1744],\n",
       " [1162],\n",
       " [1193, 1834],\n",
       " [476],\n",
       " [1273],\n",
       " [1746],\n",
       " [1576],\n",
       " [274],\n",
       " [1168],\n",
       " [1107, 651],\n",
       " [331],\n",
       " [776, 80, 846],\n",
       " [1845],\n",
       " [331],\n",
       " [1834],\n",
       " [1858, 634],\n",
       " [776, 437],\n",
       " ...]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[q_word2idx[w] for w in qs.split(' ')] for qs, a in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('there', 'where'),\n",
       " ('wow', 'lets go'),\n",
       " ('she okay', 'i hope so'),\n",
       " ('who', 'joey'),\n",
       " ('its more', 'expensive'),\n",
       " ('thirtytwo', 'get out'),\n",
       " ('okay', 'im fine im'),\n",
       " ('sure i do', 'why'),\n",
       " ('who', 'dorsey'),\n",
       " ('dorsey', 'i hate him'),\n",
       " ('do what', 'this'),\n",
       " ('who', 'bianca'),\n",
       " ('you re so', 'pleasant'),\n",
       " ('wholesome', 'unwelcome'),\n",
       " ('fallacy', 'the duck'),\n",
       " ('the duck', 'hearsay'),\n",
       " ('no', 'no what'),\n",
       " ('you think', 'oh yeah'),\n",
       " ('say it', 'what'),\n",
       " ('no', 'no'),\n",
       " ('thousands', 'why'),\n",
       " ('no no way', 'but its'),\n",
       " ('pretty', 'hmmmm'),\n",
       " ('alright', 'alright'),\n",
       " ('alright', 'alright'),\n",
       " ('alright', 'alright'),\n",
       " ('alright', 'okay'),\n",
       " ('hey honey', 'hey'),\n",
       " ('thats good', 'yeah'),\n",
       " ('how was it', 'not good'),\n",
       " ('really', 'yes'),\n",
       " ('hal', 'yes'),\n",
       " ('name', 'hammond'),\n",
       " ('yeah', 'vodka'),\n",
       " ('jack', 'yeah'),\n",
       " ('hello', 'hi its me'),\n",
       " ('hi its me', 'fuck you'),\n",
       " ('hey', 'shut up'),\n",
       " ('were saved', 'im fucked'),\n",
       " ('good luck', 'of course'),\n",
       " ('6 5', 'found it'),\n",
       " ('shit', 'what is it'),\n",
       " ('cornelius', 'oh god'),\n",
       " ('which one', 'all 900'),\n",
       " ('apipoussan', 'apipoussan'),\n",
       " ('uh', 'hoppihoppa'),\n",
       " ('like love', 'exactly'),\n",
       " ('tell me', 'i love you'),\n",
       " ('i love you', 'i love you'),\n",
       " ('hello', 'im here'),\n",
       " ('heywake up', 'huh'),\n",
       " ('sobering', 'very funny'),\n",
       " ('very funny', 'alice'),\n",
       " ('but dad', 'now'),\n",
       " ('whos jacob', 'my baby'),\n",
       " ('poor woman', 'no shit'),\n",
       " ('vomit', 'faint'),\n",
       " ('really', 'they said'),\n",
       " ('they said', 'hmm'),\n",
       " ('safety on', 'yeah'),\n",
       " ('yeah', 'this way'),\n",
       " ('do what', 'you know'),\n",
       " ('what then', 'its stupid'),\n",
       " ('its stupid', 'its fun'),\n",
       " ('no', 'no what'),\n",
       " ('yes', 'jesus why'),\n",
       " ('okay', 'fine'),\n",
       " ('good who', 'mel gordon'),\n",
       " ('you okay', 'yeah'),\n",
       " ('you okay', 'yeah'),\n",
       " ('forget it', 'im sorry'),\n",
       " ('just do it', 'attago'),\n",
       " ('yeah', 'bang'),\n",
       " ('bang', 'you and me'),\n",
       " ('rolfe', 'wade'),\n",
       " ('who', 'twombley'),\n",
       " ('twombley', 'no shit'),\n",
       " ('mom', 'yes dear'),\n",
       " ('alice', 'daddy'),\n",
       " ('daddy', 'alice i'),\n",
       " ('got it', 'stay cool'),\n",
       " ('nine', 'any of us'),\n",
       " ('four', 'jim'),\n",
       " ('jim', 'three'),\n",
       " ('one', 'but'),\n",
       " ('shepherd', 'sir'),\n",
       " ('oveur', 'dunn'),\n",
       " ('dunn', 'sir'),\n",
       " ('elaine', 'te'),\n",
       " ('elaine', 'ted'),\n",
       " ('set', 'now'),\n",
       " ('now', 'compute'),\n",
       " ('youre sure', 'im sure'),\n",
       " ('elaine', 'ted'),\n",
       " ('ted', 'yes'),\n",
       " ('hi', 'im randy'),\n",
       " ('roger', 'huh'),\n",
       " ('roger', 'huh'),\n",
       " ('you okay', 'yeah'),\n",
       " ('in there', 'show me')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'<pad>': 0,\n",
       "  'there': 1,\n",
       "  'wow': 2,\n",
       "  'she okay': 3,\n",
       "  'who': 76,\n",
       "  'its more': 5,\n",
       "  'thirtytwo': 6,\n",
       "  'okay': 67,\n",
       "  'sure i do': 8,\n",
       "  'dorsey': 10,\n",
       "  'do what': 62,\n",
       "  'you re so': 13,\n",
       "  'wholesome': 14,\n",
       "  'fallacy': 15,\n",
       "  'the duck': 16,\n",
       "  'no': 65,\n",
       "  'you think': 18,\n",
       "  'say it': 19,\n",
       "  'thousands': 21,\n",
       "  'no no way': 22,\n",
       "  'pretty': 23,\n",
       "  'alright': 27,\n",
       "  'hey honey': 28,\n",
       "  'thats good': 29,\n",
       "  'how was it': 30,\n",
       "  'really': 58,\n",
       "  'hal': 32,\n",
       "  'name': 33,\n",
       "  'yeah': 73,\n",
       "  'jack': 35,\n",
       "  'hello': 50,\n",
       "  'hi its me': 37,\n",
       "  'hey': 38,\n",
       "  'were saved': 39,\n",
       "  'good luck': 40,\n",
       "  '6 5': 41,\n",
       "  'shit': 42,\n",
       "  'cornelius': 43,\n",
       "  'which one': 44,\n",
       "  'apipoussan': 45,\n",
       "  'uh': 46,\n",
       "  'like love': 47,\n",
       "  'tell me': 48,\n",
       "  'i love you': 49,\n",
       "  'heywake up': 51,\n",
       "  'sobering': 52,\n",
       "  'very funny': 53,\n",
       "  'but dad': 54,\n",
       "  'whos jacob': 55,\n",
       "  'poor woman': 56,\n",
       "  'vomit': 57,\n",
       "  'they said': 59,\n",
       "  'safety on': 60,\n",
       "  'what then': 63,\n",
       "  'its stupid': 64,\n",
       "  'yes': 66,\n",
       "  'good who': 68,\n",
       "  'you okay': 99,\n",
       "  'forget it': 71,\n",
       "  'just do it': 72,\n",
       "  'bang': 74,\n",
       "  'rolfe': 75,\n",
       "  'twombley': 77,\n",
       "  'mom': 78,\n",
       "  'alice': 79,\n",
       "  'daddy': 80,\n",
       "  'got it': 81,\n",
       "  'nine': 82,\n",
       "  'four': 83,\n",
       "  'jim': 84,\n",
       "  'one': 85,\n",
       "  'shepherd': 86,\n",
       "  'oveur': 87,\n",
       "  'dunn': 88,\n",
       "  'elaine': 94,\n",
       "  'set': 91,\n",
       "  'now': 92,\n",
       "  'youre sure': 93,\n",
       "  'ted': 95,\n",
       "  'hi': 96,\n",
       "  'roger': 98,\n",
       "  'in there': 100},\n",
       " {0: '<pad>',\n",
       "  1: 'there',\n",
       "  2: 'wow',\n",
       "  3: 'she okay',\n",
       "  76: 'who',\n",
       "  5: 'its more',\n",
       "  6: 'thirtytwo',\n",
       "  67: 'okay',\n",
       "  8: 'sure i do',\n",
       "  10: 'dorsey',\n",
       "  62: 'do what',\n",
       "  13: 'you re so',\n",
       "  14: 'wholesome',\n",
       "  15: 'fallacy',\n",
       "  16: 'the duck',\n",
       "  65: 'no',\n",
       "  18: 'you think',\n",
       "  19: 'say it',\n",
       "  21: 'thousands',\n",
       "  22: 'no no way',\n",
       "  23: 'pretty',\n",
       "  27: 'alright',\n",
       "  28: 'hey honey',\n",
       "  29: 'thats good',\n",
       "  30: 'how was it',\n",
       "  58: 'really',\n",
       "  32: 'hal',\n",
       "  33: 'name',\n",
       "  73: 'yeah',\n",
       "  35: 'jack',\n",
       "  50: 'hello',\n",
       "  37: 'hi its me',\n",
       "  38: 'hey',\n",
       "  39: 'were saved',\n",
       "  40: 'good luck',\n",
       "  41: '6 5',\n",
       "  42: 'shit',\n",
       "  43: 'cornelius',\n",
       "  44: 'which one',\n",
       "  45: 'apipoussan',\n",
       "  46: 'uh',\n",
       "  47: 'like love',\n",
       "  48: 'tell me',\n",
       "  49: 'i love you',\n",
       "  51: 'heywake up',\n",
       "  52: 'sobering',\n",
       "  53: 'very funny',\n",
       "  54: 'but dad',\n",
       "  55: 'whos jacob',\n",
       "  56: 'poor woman',\n",
       "  57: 'vomit',\n",
       "  59: 'they said',\n",
       "  60: 'safety on',\n",
       "  63: 'what then',\n",
       "  64: 'its stupid',\n",
       "  66: 'yes',\n",
       "  68: 'good who',\n",
       "  99: 'you okay',\n",
       "  71: 'forget it',\n",
       "  72: 'just do it',\n",
       "  74: 'bang',\n",
       "  75: 'rolfe',\n",
       "  77: 'twombley',\n",
       "  78: 'mom',\n",
       "  79: 'alice',\n",
       "  80: 'daddy',\n",
       "  81: 'got it',\n",
       "  82: 'nine',\n",
       "  83: 'four',\n",
       "  84: 'jim',\n",
       "  85: 'one',\n",
       "  86: 'shepherd',\n",
       "  87: 'oveur',\n",
       "  88: 'dunn',\n",
       "  94: 'elaine',\n",
       "  91: 'set',\n",
       "  92: 'now',\n",
       "  93: 'youre sure',\n",
       "  95: 'ted',\n",
       "  96: 'hi',\n",
       "  98: 'roger',\n",
       "  100: 'in there'},\n",
       " ['5',\n",
       "  '6',\n",
       "  'alice',\n",
       "  'alright',\n",
       "  'apipoussan',\n",
       "  'bang',\n",
       "  'but',\n",
       "  'cornelius',\n",
       "  'dad',\n",
       "  'daddy',\n",
       "  'do',\n",
       "  'dorsey',\n",
       "  'duck',\n",
       "  'dunn',\n",
       "  'elaine',\n",
       "  'fallacy',\n",
       "  'forget',\n",
       "  'four',\n",
       "  'funny',\n",
       "  'good',\n",
       "  'got',\n",
       "  'hal',\n",
       "  'hello',\n",
       "  'hey',\n",
       "  'heywake',\n",
       "  'hi',\n",
       "  'honey',\n",
       "  'how',\n",
       "  'i',\n",
       "  'in',\n",
       "  'it',\n",
       "  'its',\n",
       "  'jack',\n",
       "  'jacob',\n",
       "  'jim',\n",
       "  'just',\n",
       "  'like',\n",
       "  'love',\n",
       "  'luck',\n",
       "  'me',\n",
       "  'mom',\n",
       "  'more',\n",
       "  'name',\n",
       "  'nine',\n",
       "  'no',\n",
       "  'now',\n",
       "  'okay',\n",
       "  'on',\n",
       "  'one',\n",
       "  'oveur',\n",
       "  'poor',\n",
       "  'pretty',\n",
       "  're',\n",
       "  'really',\n",
       "  'roger',\n",
       "  'rolfe',\n",
       "  'safety',\n",
       "  'said',\n",
       "  'saved',\n",
       "  'say',\n",
       "  'set',\n",
       "  'she',\n",
       "  'shepherd',\n",
       "  'shit',\n",
       "  'so',\n",
       "  'sobering',\n",
       "  'stupid',\n",
       "  'sure',\n",
       "  'ted',\n",
       "  'tell',\n",
       "  'thats',\n",
       "  'the',\n",
       "  'then',\n",
       "  'there',\n",
       "  'they',\n",
       "  'think',\n",
       "  'thirtytwo',\n",
       "  'thousands',\n",
       "  'twombley',\n",
       "  'uh',\n",
       "  'up',\n",
       "  'very',\n",
       "  'vomit',\n",
       "  'was',\n",
       "  'way',\n",
       "  'were',\n",
       "  'what',\n",
       "  'which',\n",
       "  'who',\n",
       "  'wholesome',\n",
       "  'whos',\n",
       "  'woman',\n",
       "  'wow',\n",
       "  'yeah',\n",
       "  'yes',\n",
       "  'you',\n",
       "  'youre'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_index([a for a,b in data[:100]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
