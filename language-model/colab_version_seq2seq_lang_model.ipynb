{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq-lang-model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvFqTCPtu0wN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "5bd9eff5-370e-4e67-9974-c3c595269652"
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "\n",
        "[x for x in device_lib.list_local_devices() if x.device_type == 'GPU']"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 14892338381\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 17462683336097990936\n",
              " physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8zmjY2UwUwy",
        "colab_type": "text"
      },
      "source": [
        "## Code checkout and set PWD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqXk5PRvbcKW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "6cf2d42d-db16-41f4-a882-b28b27620c5f"
      },
      "source": [
        "! git clone https://github.com/nabacg/aml-nlp-notes.git"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'aml-nlp-notes'...\n",
            "remote: Enumerating objects: 76, done.\u001b[K\n",
            "remote: Counting objects: 100% (76/76), done.\u001b[K\n",
            "remote: Compressing objects: 100% (57/57), done.\u001b[K\n",
            "remote: Total 76 (delta 19), reused 70 (delta 15), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (76/76), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sysYudfZcbki",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e6da12e6-4503-40e8-a893-fae6de89554a"
      },
      "source": [
        "import os \n",
        "print(os.getcwd())\n",
        "[n for n in dir(os) if 'ch' in n]\n",
        "os.chdir('/content/aml-nlp-notes/language-model')\n",
        "print(os.getcwd())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "/content/aml-nlp-notes/language-model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5e5jgbebQxv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "b187017a-035f-4b7c-aa55-be7cbd4fdf03"
      },
      "source": [
        "# Run this cell to mount your Google Drive.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-44V7SdwiC0",
        "colab_type": "text"
      },
      "source": [
        "### Download data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A89G_YsbbBck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! cat download_cornell.sh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "l1RdJMftbBcp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "outputId": "efdf7b69-10f0-40c9-a07e-e05da14d6dec"
      },
      "source": [
        "! sh download_cornell.sh"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-06-24 09:12:51--  https://github.com/Conchylicultor/DeepQA/raw/master/data/cornell/movie_conversations.txt\n",
            "Resolving github.com (github.com)... 192.30.253.113\n",
            "Connecting to github.com (github.com)|192.30.253.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/Conchylicultor/DeepQA/master/data/cornell/movie_conversations.txt [following]\n",
            "--2019-06-24 09:12:51--  https://raw.githubusercontent.com/Conchylicultor/DeepQA/master/data/cornell/movie_conversations.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6760930 (6.4M) [text/plain]\n",
            "Saving to: ‘movie_conversations.txt’\n",
            "\n",
            "movie_conversations 100%[===================>]   6.45M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2019-06-24 09:12:51 (68.5 MB/s) - ‘movie_conversations.txt’ saved [6760930/6760930]\n",
            "\n",
            "--2019-06-24 09:12:51--  https://github.com/Conchylicultor/DeepQA/raw/master/data/cornell/movie_lines.txt\n",
            "Resolving github.com (github.com)... 192.30.253.113\n",
            "Connecting to github.com (github.com)|192.30.253.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/Conchylicultor/DeepQA/master/data/cornell/movie_lines.txt [following]\n",
            "--2019-06-24 09:12:52--  https://raw.githubusercontent.com/Conchylicultor/DeepQA/master/data/cornell/movie_lines.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 34641919 (33M) [text/plain]\n",
            "Saving to: ‘movie_lines.txt’\n",
            "\n",
            "movie_lines.txt     100%[===================>]  33.04M   114MB/s    in 0.3s    \n",
            "\n",
            "2019-06-24 09:12:52 (114 MB/s) - ‘movie_lines.txt’ saved [34641919/34641919]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "kK3FSmx5bBcv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "fd3d9fc6-fcb0-426a-cfe7-6f1fc5580e18"
      },
      "source": [
        "! sh download_opensubs.sh"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-06-24 09:15:06--  http://opus.lingfil.uu.se/download.php?f=OpenSubtitles/en.tar.gz\n",
            "Resolving opus.lingfil.uu.se (opus.lingfil.uu.se)... 130.238.78.148\n",
            "Connecting to opus.lingfil.uu.se (opus.lingfil.uu.se)|130.238.78.148|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://opus.nlpl.eu/download.php?f=OpenSubtitles/en.tar.gz [following]\n",
            "--2019-06-24 09:15:07--  http://opus.nlpl.eu/download.php?f=OpenSubtitles/en.tar.gz\n",
            "Resolving opus.nlpl.eu (opus.nlpl.eu)... 193.166.25.9\n",
            "Connecting to opus.nlpl.eu (opus.nlpl.eu)|193.166.25.9|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://object.pouta.csc.fi/OPUS-OpenSubtitles/v1/xml/en.zip [following]\n",
            "--2019-06-24 09:15:07--  https://object.pouta.csc.fi/OPUS-OpenSubtitles/v1/xml/en.zip\n",
            "Resolving object.pouta.csc.fi (object.pouta.csc.fi)... 86.50.254.0\n",
            "Connecting to object.pouta.csc.fi (object.pouta.csc.fi)|86.50.254.0|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 148967861 (142M) [application/zip]\n",
            "Saving to: ‘en.tar.gz’\n",
            "\n",
            "en.tar.gz           100%[===================>] 142.07M  19.2MB/s    in 8.5s    \n",
            "\n",
            "2019-06-24 09:15:16 (16.7 MB/s) - ‘en.tar.gz’ saved [148967861/148967861]\n",
            "\n",
            "Archive:  en.tar.gz\n",
            "replace INFO? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace README? [y]es, [n]o, [A]ll, [N]one, [r]ename: no\n",
            "replace LICENSE? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sqCqFNebBcz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! cat download_opensubs.sh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLm0dZubdtQM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! wget -O en.tar.gz http://opus.lingfil.uu.se/download.php?f=OpenSubtitles/en.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35NMc_c0eLNq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! unzip en.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBa1blItePpG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! mv OpenSubtitles/ data/opensubs/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbIBRfXVfjpw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a563acf9-33f8-4335-d6ea-a2cbe7f81a40"
      },
      "source": [
        "! ls data/opensubs/OpenSubtitles/"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "xml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkKuQ6Edf1lI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "f89bfbdb-5e5a-4a57-d0b4-28ed7c4e3976"
      },
      "source": [
        "dataset = datasets.OpensubsData('data/opensubs')\n",
        "conversations = dataset.getConversations()\n",
        "len(conversations)\n",
        "\n",
        "# xml_doc = dataset.getXML('data/opensubs/OpenSubtitles/xml/en/Drama/1964/3322_150841_205765_alexis_zorbas.xml')\n",
        "# xml_doc = dataset.getXML('data/opensubs/OpenSubtitles/xml/en/Drama/1964/1953_216962_285208_onibaba.info')\n",
        "\n",
        "# dataset.genList(xml_doc)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OpenSubtitles data files:   0%|          | 3/4634 [00:00<03:47, 20.37it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading OpenSubtitles conversations in data/opensubs.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "OpenSubtitles data files:   4%|▍         | 197/4634 [00:06<02:40, 27.57it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Skipping file data/opensubs/OpenSubtitles/xml/en/Drama/2002/3265_149497_204017_unfaithful.xml with errors.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "OpenSubtitles data files:   9%|▉         | 408/4634 [00:15<02:33, 27.53it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Skipping file data/opensubs/OpenSubtitles/xml/en/Drama/2003/1723_68784_89159_big_fish.xml with errors.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "OpenSubtitles data files:  15%|█▌        | 698/4634 [00:27<03:01, 21.64it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Skipping file data/opensubs/OpenSubtitles/xml/en/Drama/2004/146_206647_272090_eternal_sunshine_of_the_spotless_mind.xml with errors.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "OpenSubtitles data files:  19%|█▊        | 863/4634 [00:35<02:26, 25.76it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Skipping file data/opensubs/OpenSubtitles/xml/en/Drama/2000/179_88528_119102_batoru_rowaiaru.xml with errors.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "OpenSubtitles data files:  33%|███▎      | 1548/4634 [01:03<01:49, 28.06it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Skipping file data/opensubs/OpenSubtitles/xml/en/Action/2003/602_152466_207871_batoru_rowaiaru_ii_rekuiemu.xml with errors.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "OpenSubtitles data files:  38%|███▊      | 1782/4634 [01:12<01:39, 28.70it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Skipping file data/opensubs/OpenSubtitles/xml/en/Action/2004/59_84873_113518_appurushdo.xml with errors.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "OpenSubtitles data files:  56%|█████▋    | 2614/4634 [01:50<01:34, 21.39it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Skipping file data/opensubs/OpenSubtitles/xml/en/Horror/1922/1166_134135_184270_nosferatu_eine_symphonie_des_grauens.xml with errors.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "OpenSubtitles data files:  63%|██████▎   | 2925/4634 [02:02<01:37, 17.56it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Skipping file data/opensubs/OpenSubtitles/xml/en/Family/2001/3935_19508_22105_cats__dogs.xml with errors.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "OpenSubtitles data files:  82%|████████▏ | 3798/4634 [02:45<00:32, 25.55it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Skipping file data/opensubs/OpenSubtitles/xml/en/Comedy/2003/529_124078_171007_how_to_lose_a_guy_in_10_days.xml with errors.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "OpenSubtitles data files:  89%|████████▊ | 4104/4634 [03:04<00:31, 16.92it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Skipping file data/opensubs/OpenSubtitles/xml/en/Comedy/2004/2480_226704_299940_little_black_book.xml with errors.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "OpenSubtitles data files: 100%|██████████| 4634/4634 [03:31<00:00, 21.91it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1648080"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4SYwvuVgV__",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len([n for n in dataset.filesInDir('data/opensubs') if not(n.endswith('.xml'))]), len([n for n in dataset.filesInDir('data/opensubs') if n.endswith('.xml')])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krt_AcjAbBc3",
        "colab_type": "text"
      },
      "source": [
        "## Datasets "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6W5W6_-KbBc4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datasets\n",
        "import argparse\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bx0z4PaAbBc8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_path = 'data/cornell'\n",
        "max_sentence_length = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPBh2GzKbBc_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "4962bb6b-9b19-472a-c1ae-3eba8be4c150"
      },
      "source": [
        "data = datasets.readCornellData(dataset_path, max_len=max_sentence_length)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-6c12adbc43de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadCornellData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_sentence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'dataset_path' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4ygmsyObBdD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "f7051c64-dfd2-416e-c27a-5706f93a8efb"
      },
      "source": [
        "data2 = datasets.readOpensubsData('data/opensubs', max_len=max_sentence_length)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OpenSubtitles data files: 100%|██████████| 4634/4634 [00:00<00:00, 1686602.29it/s]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading OpenSubtitles conversations in data/opensubs.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPbW5mQRbBdQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = [(preprocess_sentence(a), preprocess_sentence(b)) for (a,b) in create_dataset(num_examples)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyaCcqmBbBdU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3887ce60-73af-4bd1-b4d8-a24c277d9de6"
      },
      "source": [
        "len(data), len(data2)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5256, 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4nnTVPZbBdX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data2[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NL_Rt718MCk0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ea4e78b7-d6ee-4d07-fa9a-e9162eaaee13"
      },
      "source": [
        "[iter(data_file)]*2"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<_io.TextIOWrapper name='data/chat_corpus/twitter_en.txt' mode='r' encoding='UTF-8'>,\n",
              " <_io.TextIOWrapper name='data/chat_corpus/twitter_en.txt' mode='r' encoding='UTF-8'>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihPktrkNPxif",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buI9ixJFPHgI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def grouped(iterable, n):\n",
        "    \"s -> (s0,s1,s2,...sn-1), (sn,sn+1,sn+2,...s2n-1), (s2n,s2n+1,s2n+2,...s3n-1), ...\"\n",
        "    return zip(*[iter(iterable)]*n)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOKam_vxLOux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import itertools as it\n",
        "# data_file =  list(open('data/chat_corpus/twitter_en.txt'))\n",
        "# data3 = [(preprocess_sentence(q), preprocess_sentence(a)) for (q,a) in zip(data_file[:-1], data_file[1:])]\n",
        "data_file =  open('data/chat_corpus/twitter_en.txt')\n",
        "data3 = [(preprocess_sentence(q), preprocess_sentence(a)) for (q,a) in grouped(data_file, 2)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSFazwhfbBdc",
        "colab_type": "text"
      },
      "source": [
        "## Data preprocessing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUp9p715bBdd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converts the unicode file to ascii\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "#     w = unicode_to_ascii(w.lower().strip())\n",
        "    w = w.lower().strip()\n",
        "    \n",
        "    # creating a space between a word and the punctuation following it\n",
        "    # eg: \"he is a boy.\" => \"he is a boy .\" \n",
        "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "    \n",
        "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "#     w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "    w = re.sub(r\"[^a-zA-Z]+\", \" \", w)\n",
        "    w = w.rstrip().strip()\n",
        "    \n",
        "    # adding a start and an end token to the sentence\n",
        "    # so that the model know when to start and stop predicting.\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pGjgADtbBdg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This class creates a word -> index mapping (e.g,. \"dad\" -> 5) and vice-versa \n",
        "# (e.g., 5 -> \"dad\") for each language,\n",
        "\n",
        "\n",
        "def create_index(phrases):   \n",
        "    \n",
        "    word2idx = {}\n",
        "    idx2word = {}\n",
        "    vocab = set()\n",
        "    \n",
        "    for phrase in phrases:\n",
        "        vocab.update(phrase.split(' '))\n",
        "    \n",
        "    vocab = sorted(vocab)\n",
        "    \n",
        "    word2idx['<pad>'] = 0\n",
        "    \n",
        "    for index, word in enumerate(vocab):\n",
        "        word2idx[word] = index + 1\n",
        "    \n",
        "    for word, index in word2idx.items():\n",
        "        idx2word[index] = word\n",
        "        \n",
        "    return word2idx, idx2word, vocab\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbocOObJVLFT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This class creates a word -> index mapping (e.g,. \"dad\" -> 5) and vice-versa \n",
        "# (e.g., 5 -> \"dad\") for each language,\n",
        "\n",
        "from collections import Counter\n",
        "UNK_WORD_INDEX = 1\n",
        "def create_index(phrases, vocab_size = 15000):   # why 15k ? because that's what they used in https://arxiv.org/pdf/1406.1078.pdf\n",
        "    word2idx = {}\n",
        "#     vocab_size = 15000 \n",
        "    idx2word = {}\n",
        "    vocab = set()\n",
        "    # https://docs.python.org/3/library/collections.html#collections.Counter\n",
        "    wordcount = Counter([p for s in phrases for p in s.split(' ')])\n",
        "\n",
        "    vocab = sorted([w for (w, c) in wordcount.most_common(vocab_size)])\n",
        "    \n",
        "    word2idx['<pad>'] = 0\n",
        "    word2idx['<unk>'] = UNK_WORD_INDEX\n",
        "    for index, word in enumerate(vocab):\n",
        "        word2idx[word] = index + 2\n",
        "    \n",
        "    for word, index in word2idx.items():\n",
        "        idx2word[index] = word\n",
        "        \n",
        "    return word2idx, idx2word, vocab\n",
        "  \n",
        "def word_to_idx(lookup, word):\n",
        "  return lookup.get(word, UNK_WORD_INDEX)\n",
        "\n",
        "def idx_to_word(lookup, idx): \n",
        "  return lookup.get(idx, '<unk>')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYt3IQj_c3gv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cbaa971d-5771-43f1-d0fb-ca70e2d9b5cc"
      },
      "source": [
        "word2idx, idx2word, vocab = create_index([p for ps in data3 for p in ps])\n",
        "len(word2idx), len(idx2word), len(vocab)"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15002, 15002, 15000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEPl0qEZbBdj",
        "colab_type": "text"
      },
      "source": [
        "## Tensorflow imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIyvptnYbBdk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "43d54a58-5e2f-4ee2-c964-dda1f697e382"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "# Import TensorFlow >= 1.10 and enable eager execution\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.14.0-rc1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2FQlsF9bBdt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def grouped(iterable, n):\n",
        "    \"s -> (s0,s1,s2,...sn-1), (sn,sn+1,sn+2,...s2n-1), (s2n,s2n+1,s2n+2,...s3n-1), ...\"\n",
        "    return zip(*[iter(iterable)]*n)\n",
        "\n",
        "def readTwitterData():\n",
        "  data_file =  open('data/chat_corpus/twitter_en.txt')\n",
        "  return [(preprocess_sentence(q), preprocess_sentence(a)) for (q,a) in grouped(data_file, 2)]\n",
        " \n",
        "def create_dataset(dataset_name, max_sentence_length):\n",
        "    dataset_path = 'data/{}'.format(dataset_name)\n",
        "\n",
        "    if dataset_name == \"cornell\":\n",
        "        data = datasets.readCornellData(dataset_path, max_len=max_sentence_length)\n",
        "    elif dataset_name == \"opensubs\":\n",
        "        data = datasets.readOpensubsData(dataset_path, max_len=max_sentence_length)\n",
        "    elif dataset_name == 'twitter':\n",
        "        data = readTwitterData()\n",
        "    else:\n",
        "        raise ValueError(\"Unrecognized dataset: {!r}\".format(dataset_name))\n",
        "    \n",
        "    return data\n",
        "\n",
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)\n",
        "\n",
        "# def preprocess_sentence(s):\n",
        "#     return '<start> ' + s + ' <end>'\n",
        "\n",
        "\n",
        "def load_dataset(dataset_name = 'cornell', max_sentence_length= 10):\n",
        "    # creating cleaned input, output pairs\n",
        "    pairs = [(preprocess_sentence(a), preprocess_sentence(b)) for (a,b) in create_dataset(dataset_name, max_sentence_length)]\n",
        "\n",
        "    # index language using the class defined above    \n",
        "    word2idx, idx2word, vocab = create_index([p for ps in pairs for p in ps])\n",
        "    # Vectorize the input and target languages\n",
        "    \n",
        "    # question sentences\n",
        "    input_tensor = [[word_to_idx(word2idx, w) for w in qs.split(' ')] for qs, a in pairs]\n",
        "    \n",
        "    # answer sentences\n",
        "    target_tensor = [[word_to_idx(word2idx, w) for w in a.split(' ')] for q, a in pairs]\n",
        "    \n",
        "    # Calculate max_length of input and output tensor\n",
        "    # Here, we'll set those to the longest sentence in the dataset\n",
        "    max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
        "    \n",
        "    # Padding the input and output tensor to the maximum length\n",
        "    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
        "                                                                 maxlen=max_length_inp,\n",
        "                                                                 padding='post')\n",
        "    \n",
        "    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
        "                                                                  maxlen=max_length_tar, \n",
        "                                                                  padding='post')\n",
        "    \n",
        "    return input_tensor, target_tensor, (word2idx, idx2word, vocab), max_length_inp, max_length_tar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MP30pUF-bBdw",
        "colab_type": "text"
      },
      "source": [
        "## Prep training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GgOagIGbBdx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "7d53cd42-b564-4e74-cf28-d77adfd2f7b1"
      },
      "source": [
        "# Try experimenting with the size of that dataset\n",
        "\n",
        "input_tensor, target_tensor, dict_index, max_length_inp, max_length_targ = load_dataset(dataset_name = 'opensubs', max_sentence_length=20)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rOpenSubtitles data files:   0%|          | 0/4638 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading OpenSubtitles conversations in data/opensubs.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "OpenSubtitles data files:   4%|▍         | 201/4638 [00:06<02:45, 26.82it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Skipping file data/opensubs/OpenSubtitles/xml/en/Drama/2002/3265_149497_204017_unfaithful.xml with errors.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "OpenSubtitles data files:   9%|▊         | 403/4638 [00:13<02:07, 33.24it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Skipping file data/opensubs/OpenSubtitles/xml/en/Drama/2003/1723_68784_89159_big_fish.xml with errors.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "OpenSubtitles data files:  15%|█▍        | 688/4638 [00:23<03:06, 21.21it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Skipping file data/opensubs/OpenSubtitles/xml/en/Drama/2004/146_206647_272090_eternal_sunshine_of_the_spotless_mind.xml with errors.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "OpenSubtitles data files:  19%|█▊        | 868/4638 [00:30<01:58, 31.90it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Skipping file data/opensubs/OpenSubtitles/xml/en/Drama/2000/179_88528_119102_batoru_rowaiaru.xml with errors.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "OpenSubtitles data files:  34%|███▎      | 1558/4638 [00:53<01:19, 38.98it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Skipping file data/opensubs/OpenSubtitles/xml/en/Action/2003/602_152466_207871_batoru_rowaiaru_ii_rekuiemu.xml with errors.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "OpenSubtitles data files:  39%|███▉      | 1800/4638 [01:01<01:20, 35.26it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Skipping file data/opensubs/OpenSubtitles/xml/en/Action/2004/59_84873_113518_appurushdo.xml with errors.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "OpenSubtitles data files:  57%|█████▋    | 2622/4638 [01:32<01:11, 28.09it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Skipping file data/opensubs/OpenSubtitles/xml/en/Horror/1922/1166_134135_184270_nosferatu_eine_symphonie_des_grauens.xml with errors.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "OpenSubtitles data files:  63%|██████▎   | 2925/4638 [01:42<00:43, 39.63it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Skipping file data/opensubs/OpenSubtitles/xml/en/Family/2001/3935_19508_22105_cats__dogs.xml with errors.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "OpenSubtitles data files:  82%|████████▏ | 3802/4638 [02:20<00:28, 29.44it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Skipping file data/opensubs/OpenSubtitles/xml/en/Comedy/2003/529_124078_171007_how_to_lose_a_guy_in_10_days.xml with errors.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "OpenSubtitles data files:  88%|████████▊ | 4098/4638 [02:36<00:15, 34.63it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Skipping file data/opensubs/OpenSubtitles/xml/en/Comedy/2004/2480_226704_299940_little_black_book.xml with errors.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "OpenSubtitles data files: 100%|██████████| 4638/4638 [03:00<00:00, 25.75it/s]\n",
            "100%|██████████| 1648080/1648080 [00:21<00:00, 75607.90it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5pbqvjbbBd3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1b340569-fb25-41f1-9d21-97b14dd46314"
      },
      "source": [
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(132853, 132853, 33214, 33214)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bj5bONni1u5T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4w6JdWjbBeA",
        "colab_type": "text"
      },
      "source": [
        "### Create TF dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lR7FF0uMe5LA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "107adc07-9580-4091-dab5-e901c32e001a"
      },
      "source": [
        "len(word2idx)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15002"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rHAd07bbBeC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_size = len(word2idx)\n",
        "\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-k8-djT3bBeF",
        "colab_type": "text"
      },
      "source": [
        "## NN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isOeDPKebBeI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gru(units):\n",
        "  # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\n",
        "  # the code automatically does that.\n",
        "    if tf.test.is_gpu_available():\n",
        "        return tf.keras.layers.CuDNNGRU(units, \n",
        "                                    return_sequences=True, \n",
        "                                    return_state=True, \n",
        "                                    recurrent_initializer='glorot_uniform')\n",
        "    else:\n",
        "        return tf.keras.layers.GRU(units, \n",
        "                               return_sequences=True, \n",
        "                               return_state=True, \n",
        "                               recurrent_activation='sigmoid', \n",
        "                               recurrent_initializer='glorot_uniform')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "575h3_QZbBeN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = gru(self.enc_units)\n",
        "        \n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        output, state = self.gru(x, initial_state = hidden)        \n",
        "        return output, state\n",
        "    \n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbRHnJ0zbBeS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = gru(self.dec_units)\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "        # used for attention\n",
        "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
        "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "        \n",
        "    def call(self, x, hidden, enc_output):\n",
        "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "        \n",
        "        # hidden shape == (batch_size, hidden size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        # we are doing this to perform addition to calculate the score\n",
        "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "        \n",
        "        # score shape == (batch_size, max_length, 1)\n",
        "        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n",
        "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
        "        \n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        \n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * enc_output\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        \n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "        \n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        \n",
        "        # passing the concatenated vector to the GRU\n",
        "        output, state = self.gru(x)\n",
        "        \n",
        "        # output shape == (batch_size * 1, hidden_size)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "        \n",
        "        # output shape == (batch_size * 1, vocab)\n",
        "        x = self.fc(output)\n",
        "        \n",
        "        return x, state, attention_weights\n",
        "        \n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.dec_units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5z0XiaImbBeU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = 1 - np.equal(real, 0)\n",
        "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
        "    return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sFoQnCLbBeX",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGjd20w5bBeX",
        "colab_type": "text"
      },
      "source": [
        "### setup "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7PZvP9BbBeY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = Encoder(vocab_size, embedding_dim, units, BATCH_SIZE)\n",
        "decoder = Decoder(vocab_size, embedding_dim, units, BATCH_SIZE)\n",
        "optimizer = tf.train.AdamOptimizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OC1FRWd5S3SV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "523cce45-b3aa-4300-d5ae-df0bc8b81812"
      },
      "source": [
        "vocab_size"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24827"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72wFA9SNbBeb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dict_index[0]['<start>']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxsPrd4vbBef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2idx = dict_index[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHBtgzzPbBei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = './training_checkpoints_new_dict'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1ikcv8FzImF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b837751a-e32f-45c9-c317-d1db73c1ed92"
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.InitializationOnlyStatus at 0x7fec75ffb128>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SA_KugO9bBen",
        "colab_type": "text"
      },
      "source": [
        "### Training explained\n",
        "\n",
        "1. Pass the *input* through the *encoder* which return *encoder output* and the *encoder hidden state*.\n",
        "2. The encoder output, encoder hidden state and the decoder input (which is the *start token*) is passed to the decoder.\n",
        "3. The decoder returns the *predictions* and the *decoder hidden state*.\n",
        "4. The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n",
        "5. Use *teacher forcing* to decide the next input to the decoder.\n",
        "6. *Teacher forcing* is the technique where the *target word* is passed as the *next input* to the decoder.\n",
        "7. The final step is to calculate the gradients and apply it to the optimizer and backpropagate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4S0zeKGbBeo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4097
        },
        "outputId": "1fb631a6-728a-4fa5-853e-39690860d9d6"
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "    \n",
        "    for (batch, (inp, targ)) in enumerate(dataset):\n",
        "        loss = 0\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            enc_output, enc_hidden = encoder(inp, hidden)\n",
        "            \n",
        "            dec_hidden = enc_hidden\n",
        "            \n",
        "            dec_input = tf.expand_dims([word_to_idx(word2idx, '<start>')] * BATCH_SIZE, 1)       \n",
        "            \n",
        "            # Teacher forcing - feeding the target as the next input\n",
        "            for t in range(1, targ.shape[1]):\n",
        "                # passing enc_output to the decoder\n",
        "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "                \n",
        "                loss += loss_function(targ[:, t], predictions)\n",
        "                \n",
        "                # using teacher forcing\n",
        "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "        \n",
        "        batch_loss = (loss / int(targ.shape[1]))\n",
        "        \n",
        "        total_loss += batch_loss\n",
        "        \n",
        "        variables = encoder.variables + decoder.variables\n",
        "        \n",
        "        gradients = tape.gradient(loss, variables)\n",
        "        \n",
        "        optimizer.apply_gradients(zip(gradients, variables))\n",
        "        \n",
        "        if batch % 100 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                         batch,\n",
        "                                                         batch_loss.numpy()))\n",
        "    # saving (checkpoint) the model every 2 epochs\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "    \n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                        total_loss / N_BATCH))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 3.3349\n",
            "Epoch 1 Batch 100 Loss 1.8099\n",
            "Epoch 1 Batch 200 Loss 1.9263\n",
            "Epoch 1 Batch 300 Loss 1.7541\n",
            "Epoch 1 Batch 400 Loss 1.8017\n",
            "Epoch 1 Batch 500 Loss 1.8506\n",
            "Epoch 1 Batch 600 Loss 1.5905\n",
            "Epoch 1 Batch 700 Loss 1.7875\n",
            "Epoch 1 Batch 800 Loss 1.6588\n",
            "Epoch 1 Batch 900 Loss 1.7806\n",
            "Epoch 1 Batch 1000 Loss 1.6489\n",
            "Epoch 1 Batch 1100 Loss 1.6366\n",
            "Epoch 1 Batch 1200 Loss 1.7313\n",
            "Epoch 1 Batch 1300 Loss 1.5661\n",
            "Epoch 1 Batch 1400 Loss 1.6592\n",
            "Epoch 1 Batch 1500 Loss 1.5575\n",
            "Epoch 1 Batch 1600 Loss 1.5542\n",
            "Epoch 1 Batch 1700 Loss 1.6314\n",
            "Epoch 1 Batch 1800 Loss 1.5234\n",
            "Epoch 1 Batch 1900 Loss 1.6759\n",
            "Epoch 1 Batch 2000 Loss 1.4091\n",
            "Epoch 1 Loss 1.6841\n",
            "Time taken for 1 epoch 942.8845384120941 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.5565\n",
            "Epoch 2 Batch 100 Loss 1.4481\n",
            "Epoch 2 Batch 200 Loss 1.5854\n",
            "Epoch 2 Batch 300 Loss 1.4812\n",
            "Epoch 2 Batch 400 Loss 1.5453\n",
            "Epoch 2 Batch 500 Loss 1.6214\n",
            "Epoch 2 Batch 600 Loss 1.4009\n",
            "Epoch 2 Batch 700 Loss 1.5412\n",
            "Epoch 2 Batch 800 Loss 1.4820\n",
            "Epoch 2 Batch 900 Loss 1.5645\n",
            "Epoch 2 Batch 1000 Loss 1.4615\n",
            "Epoch 2 Batch 1100 Loss 1.4557\n",
            "Epoch 2 Batch 1200 Loss 1.5332\n",
            "Epoch 2 Batch 1300 Loss 1.4462\n",
            "Epoch 2 Batch 1400 Loss 1.4997\n",
            "Epoch 2 Batch 1500 Loss 1.4319\n",
            "Epoch 2 Batch 1600 Loss 1.4029\n",
            "Epoch 2 Batch 1700 Loss 1.4706\n",
            "Epoch 2 Batch 1800 Loss 1.3794\n",
            "Epoch 2 Batch 1900 Loss 1.5062\n",
            "Epoch 2 Batch 2000 Loss 1.2784\n",
            "Epoch 2 Loss 1.4719\n",
            "Time taken for 1 epoch 943.8250684738159 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.3813\n",
            "Epoch 3 Batch 100 Loss 1.3381\n",
            "Epoch 3 Batch 200 Loss 1.4555\n",
            "Epoch 3 Batch 300 Loss 1.3584\n",
            "Epoch 3 Batch 400 Loss 1.4474\n",
            "Epoch 3 Batch 500 Loss 1.5067\n",
            "Epoch 3 Batch 600 Loss 1.2937\n",
            "Epoch 3 Batch 700 Loss 1.4280\n",
            "Epoch 3 Batch 800 Loss 1.3781\n",
            "Epoch 3 Batch 900 Loss 1.4550\n",
            "Epoch 3 Batch 1000 Loss 1.3602\n",
            "Epoch 3 Batch 1100 Loss 1.3525\n",
            "Epoch 3 Batch 1200 Loss 1.4381\n",
            "Epoch 3 Batch 1300 Loss 1.3779\n",
            "Epoch 3 Batch 1400 Loss 1.3954\n",
            "Epoch 3 Batch 1500 Loss 1.3232\n",
            "Epoch 3 Batch 1600 Loss 1.2966\n",
            "Epoch 3 Batch 1700 Loss 1.3765\n",
            "Epoch 3 Batch 1800 Loss 1.2887\n",
            "Epoch 3 Batch 1900 Loss 1.3766\n",
            "Epoch 3 Batch 2000 Loss 1.1927\n",
            "Epoch 3 Loss 1.3638\n",
            "Time taken for 1 epoch 965.2258148193359 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.2602\n",
            "Epoch 4 Batch 100 Loss 1.2126\n",
            "Epoch 4 Batch 200 Loss 1.3358\n",
            "Epoch 4 Batch 300 Loss 1.2670\n",
            "Epoch 4 Batch 400 Loss 1.3379\n",
            "Epoch 4 Batch 500 Loss 1.3977\n",
            "Epoch 4 Batch 600 Loss 1.2209\n",
            "Epoch 4 Batch 700 Loss 1.3356\n",
            "Epoch 4 Batch 800 Loss 1.2765\n",
            "Epoch 4 Batch 900 Loss 1.3331\n",
            "Epoch 4 Batch 1000 Loss 1.2881\n",
            "Epoch 4 Batch 1100 Loss 1.2521\n",
            "Epoch 4 Batch 1200 Loss 1.3405\n",
            "Epoch 4 Batch 1300 Loss 1.3016\n",
            "Epoch 4 Batch 1400 Loss 1.3042\n",
            "Epoch 4 Batch 1500 Loss 1.2117\n",
            "Epoch 4 Batch 1600 Loss 1.2063\n",
            "Epoch 4 Batch 1700 Loss 1.2671\n",
            "Epoch 4 Batch 1800 Loss 1.1939\n",
            "Epoch 4 Batch 1900 Loss 1.2646\n",
            "Epoch 4 Batch 2000 Loss 1.1016\n",
            "Epoch 4 Loss 1.2603\n",
            "Time taken for 1 epoch 954.2657053470612 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.1392\n",
            "Epoch 5 Batch 100 Loss 1.0488\n",
            "Epoch 5 Batch 200 Loss 1.2069\n",
            "Epoch 5 Batch 300 Loss 1.1918\n",
            "Epoch 5 Batch 400 Loss 1.2241\n",
            "Epoch 5 Batch 500 Loss 1.2558\n",
            "Epoch 5 Batch 600 Loss 1.1281\n",
            "Epoch 5 Batch 700 Loss 1.2213\n",
            "Epoch 5 Batch 800 Loss 1.1683\n",
            "Epoch 5 Batch 900 Loss 1.2145\n",
            "Epoch 5 Batch 1000 Loss 1.1985\n",
            "Epoch 5 Batch 1100 Loss 1.1520\n",
            "Epoch 5 Batch 1200 Loss 1.2193\n",
            "Epoch 5 Batch 1300 Loss 1.1992\n",
            "Epoch 5 Batch 1400 Loss 1.1933\n",
            "Epoch 5 Batch 1500 Loss 1.0997\n",
            "Epoch 5 Batch 1600 Loss 1.1280\n",
            "Epoch 5 Batch 1700 Loss 1.1652\n",
            "Epoch 5 Batch 1800 Loss 1.1010\n",
            "Epoch 5 Batch 1900 Loss 1.1727\n",
            "Epoch 5 Batch 2000 Loss 1.0245\n",
            "Epoch 5 Loss 1.1517\n",
            "Time taken for 1 epoch 951.1112906932831 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.0106\n",
            "Epoch 6 Batch 100 Loss 0.9188\n",
            "Epoch 6 Batch 200 Loss 1.1006\n",
            "Epoch 6 Batch 300 Loss 1.0964\n",
            "Epoch 6 Batch 400 Loss 1.0924\n",
            "Epoch 6 Batch 500 Loss 1.1301\n",
            "Epoch 6 Batch 600 Loss 1.0109\n",
            "Epoch 6 Batch 700 Loss 1.1227\n",
            "Epoch 6 Batch 800 Loss 1.0612\n",
            "Epoch 6 Batch 900 Loss 1.0997\n",
            "Epoch 6 Batch 1000 Loss 1.1104\n",
            "Epoch 6 Batch 1100 Loss 1.0696\n",
            "Epoch 6 Batch 1200 Loss 1.1296\n",
            "Epoch 6 Batch 1300 Loss 1.0835\n",
            "Epoch 6 Batch 1400 Loss 1.0831\n",
            "Epoch 6 Batch 1500 Loss 0.9685\n",
            "Epoch 6 Batch 1600 Loss 1.0560\n",
            "Epoch 6 Batch 1700 Loss 1.0467\n",
            "Epoch 6 Batch 1800 Loss 1.0147\n",
            "Epoch 6 Batch 1900 Loss 1.0552\n",
            "Epoch 6 Batch 2000 Loss 0.9198\n",
            "Epoch 6 Loss 1.0452\n",
            "Time taken for 1 epoch 956.1510081291199 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.8802\n",
            "Epoch 7 Batch 100 Loss 0.8102\n",
            "Epoch 7 Batch 200 Loss 0.9785\n",
            "Epoch 7 Batch 300 Loss 1.0125\n",
            "Epoch 7 Batch 400 Loss 0.9862\n",
            "Epoch 7 Batch 500 Loss 1.0284\n",
            "Epoch 7 Batch 600 Loss 0.9060\n",
            "Epoch 7 Batch 700 Loss 1.0320\n",
            "Epoch 7 Batch 800 Loss 0.9821\n",
            "Epoch 7 Batch 900 Loss 0.9978\n",
            "Epoch 7 Batch 1000 Loss 1.0324\n",
            "Epoch 7 Batch 1100 Loss 0.9712\n",
            "Epoch 7 Batch 1200 Loss 1.0210\n",
            "Epoch 7 Batch 1300 Loss 0.9702\n",
            "Epoch 7 Batch 1400 Loss 0.9585\n",
            "Epoch 7 Batch 1500 Loss 0.8504\n",
            "Epoch 7 Batch 1600 Loss 0.9542\n",
            "Epoch 7 Batch 1700 Loss 0.9410\n",
            "Epoch 7 Batch 1800 Loss 0.9391\n",
            "Epoch 7 Batch 1900 Loss 0.9443\n",
            "Epoch 7 Batch 2000 Loss 0.8308\n",
            "Epoch 7 Loss 0.9443\n",
            "Time taken for 1 epoch 953.7844400405884 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.7671\n",
            "Epoch 8 Batch 100 Loss 0.7121\n",
            "Epoch 8 Batch 200 Loss 0.8502\n",
            "Epoch 8 Batch 300 Loss 0.9390\n",
            "Epoch 8 Batch 400 Loss 0.8822\n",
            "Epoch 8 Batch 500 Loss 0.9177\n",
            "Epoch 8 Batch 600 Loss 0.8240\n",
            "Epoch 8 Batch 700 Loss 0.9463\n",
            "Epoch 8 Batch 800 Loss 0.9077\n",
            "Epoch 8 Batch 900 Loss 0.8809\n",
            "Epoch 8 Batch 1000 Loss 0.9482\n",
            "Epoch 8 Batch 1100 Loss 0.8696\n",
            "Epoch 8 Batch 1200 Loss 0.8951\n",
            "Epoch 8 Batch 1300 Loss 0.8716\n",
            "Epoch 8 Batch 1400 Loss 0.8551\n",
            "Epoch 8 Batch 1500 Loss 0.7386\n",
            "Epoch 8 Batch 1600 Loss 0.9009\n",
            "Epoch 8 Batch 1700 Loss 0.8318\n",
            "Epoch 8 Batch 1800 Loss 0.8729\n",
            "Epoch 8 Batch 1900 Loss 0.8480\n",
            "Epoch 8 Batch 2000 Loss 0.7338\n",
            "Epoch 8 Loss 0.8506\n",
            "Time taken for 1 epoch 949.0509850978851 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.6646\n",
            "Epoch 9 Batch 100 Loss 0.6360\n",
            "Epoch 9 Batch 200 Loss 0.7589\n",
            "Epoch 9 Batch 300 Loss 0.8672\n",
            "Epoch 9 Batch 400 Loss 0.8024\n",
            "Epoch 9 Batch 500 Loss 0.8250\n",
            "Epoch 9 Batch 600 Loss 0.7360\n",
            "Epoch 9 Batch 700 Loss 0.8668\n",
            "Epoch 9 Batch 800 Loss 0.8440\n",
            "Epoch 9 Batch 900 Loss 0.8027\n",
            "Epoch 9 Batch 1000 Loss 0.8262\n",
            "Epoch 9 Batch 1100 Loss 0.7802\n",
            "Epoch 9 Batch 1200 Loss 0.7856\n",
            "Epoch 9 Batch 1300 Loss 0.7905\n",
            "Epoch 9 Batch 1400 Loss 0.7557\n",
            "Epoch 9 Batch 1500 Loss 0.6068\n",
            "Epoch 9 Batch 1600 Loss 0.8040\n",
            "Epoch 9 Batch 1700 Loss 0.7658\n",
            "Epoch 9 Batch 1800 Loss 0.8082\n",
            "Epoch 9 Batch 1900 Loss 0.7733\n",
            "Epoch 9 Batch 2000 Loss 0.6708\n",
            "Epoch 9 Loss 0.7660\n",
            "Time taken for 1 epoch 925.4725949764252 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.5996\n",
            "Epoch 10 Batch 100 Loss 0.5693\n",
            "Epoch 10 Batch 200 Loss 0.7001\n",
            "Epoch 10 Batch 300 Loss 0.7766\n",
            "Epoch 10 Batch 400 Loss 0.7512\n",
            "Epoch 10 Batch 500 Loss 0.7521\n",
            "Epoch 10 Batch 600 Loss 0.6664\n",
            "Epoch 10 Batch 700 Loss 0.7644\n",
            "Epoch 10 Batch 800 Loss 0.7583\n",
            "Epoch 10 Batch 900 Loss 0.7388\n",
            "Epoch 10 Batch 1000 Loss 0.7935\n",
            "Epoch 10 Batch 1100 Loss 0.6831\n",
            "Epoch 10 Batch 1200 Loss 0.6920\n",
            "Epoch 10 Batch 1300 Loss 0.7344\n",
            "Epoch 10 Batch 1400 Loss 0.6824\n",
            "Epoch 10 Batch 1500 Loss 0.5222\n",
            "Epoch 10 Batch 1600 Loss 0.6884\n",
            "Epoch 10 Batch 1700 Loss 0.6841\n",
            "Epoch 10 Batch 1800 Loss 0.7544\n",
            "Epoch 10 Batch 1900 Loss 0.6964\n",
            "Epoch 10 Batch 2000 Loss 0.6059\n",
            "Epoch 10 Loss 0.6904\n",
            "Time taken for 1 epoch 922.0955126285553 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WtYt8teMfEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! cp /content/aml-nlp-notes/language-model/training_checkpoints/* /content/drive/'My Drive'/NLP-checkpoints"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBdb6bO5NAJv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! ls /content/drive/'My Drive'/NLP-checkpoints"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMMLnE4LzVKS",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:22012/content/aml-nlp-notes/language-model/training_checkpoints": {
              "data": "CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNTAxIChOb3QgSW1wbGVtZW50ZWQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj41MDEuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K",
              "ok": false,
              "headers": [
                [
                  "content-length",
                  "1455"
                ],
                [
                  "content-type",
                  "text/html; charset=utf-8"
                ]
              ],
              "status": 501,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "outputId": "3665bc55-4cd1-4913-d661-7721d8a350f2"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('/content/aml-nlp-notes/language-model/training_checkpoints')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-31783069c46b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/aml-nlp-notes/language-model/training_checkpoints'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0;34m'port'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m       \u001b[0;34m'path'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m       \u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m   })\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: Failed to download: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzhL7vKAbBeq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4097
        },
        "outputId": "1aad5dd2-d2bb-44d2-d596-86347a87e40d"
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "    \n",
        "    for (batch, (inp, targ)) in enumerate(dataset):\n",
        "        loss = 0\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            enc_output, enc_hidden = encoder(inp, hidden)\n",
        "            \n",
        "            dec_hidden = enc_hidden\n",
        "            \n",
        "            dec_input = tf.expand_dims([word_to_idx(word2idx, '<start>')] * BATCH_SIZE, 1)       \n",
        "            \n",
        "            # Teacher forcing - feeding the target as the next input\n",
        "            for t in range(1, targ.shape[1]):\n",
        "                # passing enc_output to the decoder\n",
        "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "                \n",
        "                loss += loss_function(targ[:, t], predictions)\n",
        "                \n",
        "                # using teacher forcing\n",
        "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "        \n",
        "        batch_loss = (loss / int(targ.shape[1]))\n",
        "        \n",
        "        total_loss += batch_loss\n",
        "        \n",
        "        variables = encoder.variables + decoder.variables\n",
        "        \n",
        "        gradients = tape.gradient(loss, variables)\n",
        "        \n",
        "        optimizer.apply_gradients(zip(gradients, variables))\n",
        "        \n",
        "        if batch % 100 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                         batch,\n",
        "                                                         batch_loss.numpy()))\n",
        "    # saving (checkpoint) the model every 2 epochs\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "    \n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                        total_loss / N_BATCH))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 0.5342\n",
            "Epoch 1 Batch 100 Loss 0.4929\n",
            "Epoch 1 Batch 200 Loss 0.5811\n",
            "Epoch 1 Batch 300 Loss 0.6868\n",
            "Epoch 1 Batch 400 Loss 0.6587\n",
            "Epoch 1 Batch 500 Loss 0.6629\n",
            "Epoch 1 Batch 600 Loss 0.5958\n",
            "Epoch 1 Batch 700 Loss 0.6935\n",
            "Epoch 1 Batch 800 Loss 0.6902\n",
            "Epoch 1 Batch 900 Loss 0.6347\n",
            "Epoch 1 Batch 1000 Loss 0.7113\n",
            "Epoch 1 Batch 1100 Loss 0.6320\n",
            "Epoch 1 Batch 1200 Loss 0.6269\n",
            "Epoch 1 Batch 1300 Loss 0.6261\n",
            "Epoch 1 Batch 1400 Loss 0.6154\n",
            "Epoch 1 Batch 1500 Loss 0.4569\n",
            "Epoch 1 Batch 1600 Loss 0.6122\n",
            "Epoch 1 Batch 1700 Loss 0.6207\n",
            "Epoch 1 Batch 1800 Loss 0.6942\n",
            "Epoch 1 Batch 1900 Loss 0.6537\n",
            "Epoch 1 Batch 2000 Loss 0.5552\n",
            "Epoch 1 Loss 0.6214\n",
            "Time taken for 1 epoch 945.6231434345245 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.4600\n",
            "Epoch 2 Batch 100 Loss 0.4493\n",
            "Epoch 2 Batch 200 Loss 0.5135\n",
            "Epoch 2 Batch 300 Loss 0.6248\n",
            "Epoch 2 Batch 400 Loss 0.5779\n",
            "Epoch 2 Batch 500 Loss 0.6144\n",
            "Epoch 2 Batch 600 Loss 0.5609\n",
            "Epoch 2 Batch 700 Loss 0.5932\n",
            "Epoch 2 Batch 800 Loss 0.6402\n",
            "Epoch 2 Batch 900 Loss 0.5845\n",
            "Epoch 2 Batch 1000 Loss 0.6402\n",
            "Epoch 2 Batch 1100 Loss 0.5506\n",
            "Epoch 2 Batch 1200 Loss 0.5564\n",
            "Epoch 2 Batch 1300 Loss 0.5476\n",
            "Epoch 2 Batch 1400 Loss 0.5512\n",
            "Epoch 2 Batch 1500 Loss 0.4140\n",
            "Epoch 2 Batch 1600 Loss 0.5669\n",
            "Epoch 2 Batch 1700 Loss 0.5557\n",
            "Epoch 2 Batch 1800 Loss 0.6238\n",
            "Epoch 2 Batch 1900 Loss 0.5767\n",
            "Epoch 2 Batch 2000 Loss 0.4881\n",
            "Epoch 2 Loss 0.5594\n",
            "Time taken for 1 epoch 937.1665861606598 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.4126\n",
            "Epoch 3 Batch 100 Loss 0.3975\n",
            "Epoch 3 Batch 200 Loss 0.4622\n",
            "Epoch 3 Batch 300 Loss 0.6504\n",
            "Epoch 3 Batch 400 Loss 0.5115\n",
            "Epoch 3 Batch 500 Loss 0.5359\n",
            "Epoch 3 Batch 600 Loss 0.5187\n",
            "Epoch 3 Batch 700 Loss 0.5182\n",
            "Epoch 3 Batch 800 Loss 0.6806\n",
            "Epoch 3 Batch 900 Loss 0.5308\n",
            "Epoch 3 Batch 1000 Loss 0.5559\n",
            "Epoch 3 Batch 1100 Loss 0.4941\n",
            "Epoch 3 Batch 1200 Loss 0.4988\n",
            "Epoch 3 Batch 1300 Loss 0.4944\n",
            "Epoch 3 Batch 1400 Loss 0.5038\n",
            "Epoch 3 Batch 1500 Loss 0.3470\n",
            "Epoch 3 Batch 1600 Loss 0.5191\n",
            "Epoch 3 Batch 1700 Loss 0.5156\n",
            "Epoch 3 Batch 1800 Loss 0.5658\n",
            "Epoch 3 Batch 1900 Loss 0.4944\n",
            "Epoch 3 Batch 2000 Loss 0.4481\n",
            "Epoch 3 Loss 0.5058\n",
            "Time taken for 1 epoch 934.0288593769073 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.3895\n",
            "Epoch 4 Batch 100 Loss 0.3270\n",
            "Epoch 4 Batch 200 Loss 0.4231\n",
            "Epoch 4 Batch 300 Loss 0.5782\n",
            "Epoch 4 Batch 400 Loss 0.4342\n",
            "Epoch 4 Batch 500 Loss 0.4904\n",
            "Epoch 4 Batch 600 Loss 0.5065\n",
            "Epoch 4 Batch 700 Loss 0.4688\n",
            "Epoch 4 Batch 800 Loss 0.5848\n",
            "Epoch 4 Batch 900 Loss 0.4636\n",
            "Epoch 4 Batch 1000 Loss 0.5199\n",
            "Epoch 4 Batch 1100 Loss 0.4343\n",
            "Epoch 4 Batch 1200 Loss 0.4656\n",
            "Epoch 4 Batch 1300 Loss 0.4290\n",
            "Epoch 4 Batch 1400 Loss 0.4546\n",
            "Epoch 4 Batch 1500 Loss 0.3234\n",
            "Epoch 4 Batch 1600 Loss 0.4904\n",
            "Epoch 4 Batch 1700 Loss 0.4621\n",
            "Epoch 4 Batch 1800 Loss 0.4909\n",
            "Epoch 4 Batch 1900 Loss 0.4627\n",
            "Epoch 4 Batch 2000 Loss 0.4197\n",
            "Epoch 4 Loss 0.4567\n",
            "Time taken for 1 epoch 939.7797932624817 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.4021\n",
            "Epoch 5 Batch 100 Loss 0.2820\n",
            "Epoch 5 Batch 200 Loss 0.4167\n",
            "Epoch 5 Batch 300 Loss 0.4949\n",
            "Epoch 5 Batch 400 Loss 0.5099\n",
            "Epoch 5 Batch 500 Loss 0.4470\n",
            "Epoch 5 Batch 600 Loss 0.5144\n",
            "Epoch 5 Batch 700 Loss 0.4328\n",
            "Epoch 5 Batch 800 Loss 0.4909\n",
            "Epoch 5 Batch 900 Loss 0.4458\n",
            "Epoch 5 Batch 1000 Loss 0.4659\n",
            "Epoch 5 Batch 1100 Loss 0.3949\n",
            "Epoch 5 Batch 1200 Loss 0.4292\n",
            "Epoch 5 Batch 1300 Loss 0.4035\n",
            "Epoch 5 Batch 1400 Loss 0.4068\n",
            "Epoch 5 Batch 1500 Loss 0.2960\n",
            "Epoch 5 Batch 1600 Loss 0.4615\n",
            "Epoch 5 Batch 1700 Loss 0.4375\n",
            "Epoch 5 Batch 1800 Loss 0.4618\n",
            "Epoch 5 Batch 1900 Loss 0.4378\n",
            "Epoch 5 Batch 2000 Loss 0.3709\n",
            "Epoch 5 Loss 0.4197\n",
            "Time taken for 1 epoch 941.5408873558044 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.3713\n",
            "Epoch 6 Batch 100 Loss 0.3047\n",
            "Epoch 6 Batch 200 Loss 0.4523\n",
            "Epoch 6 Batch 300 Loss 0.4794\n",
            "Epoch 6 Batch 400 Loss 0.4238\n",
            "Epoch 6 Batch 500 Loss 0.4353\n",
            "Epoch 6 Batch 600 Loss 0.4625\n",
            "Epoch 6 Batch 700 Loss 0.3856\n",
            "Epoch 6 Batch 800 Loss 0.4501\n",
            "Epoch 6 Batch 900 Loss 0.4053\n",
            "Epoch 6 Batch 1000 Loss 0.4163\n",
            "Epoch 6 Batch 1100 Loss 0.3536\n",
            "Epoch 6 Batch 1200 Loss 0.3727\n",
            "Epoch 6 Batch 1300 Loss 0.4106\n",
            "Epoch 6 Batch 1400 Loss 0.3898\n",
            "Epoch 6 Batch 1500 Loss 0.3151\n",
            "Epoch 6 Batch 1600 Loss 0.4467\n",
            "Epoch 6 Batch 1700 Loss 0.4156\n",
            "Epoch 6 Batch 1800 Loss 0.4195\n",
            "Epoch 6 Batch 1900 Loss 0.3701\n",
            "Epoch 6 Batch 2000 Loss 0.3234\n",
            "Epoch 6 Loss 0.3879\n",
            "Time taken for 1 epoch 947.4927294254303 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.3346\n",
            "Epoch 7 Batch 100 Loss 0.2566\n",
            "Epoch 7 Batch 200 Loss 0.3918\n",
            "Epoch 7 Batch 300 Loss 0.4444\n",
            "Epoch 7 Batch 400 Loss 0.3764\n",
            "Epoch 7 Batch 500 Loss 0.4101\n",
            "Epoch 7 Batch 600 Loss 0.4080\n",
            "Epoch 7 Batch 700 Loss 0.3582\n",
            "Epoch 7 Batch 800 Loss 0.4325\n",
            "Epoch 7 Batch 900 Loss 0.3769\n",
            "Epoch 7 Batch 1000 Loss 0.3864\n",
            "Epoch 7 Batch 1100 Loss 0.2933\n",
            "Epoch 7 Batch 1200 Loss 0.3600\n",
            "Epoch 7 Batch 1300 Loss 0.3746\n",
            "Epoch 7 Batch 1400 Loss 0.3564\n",
            "Epoch 7 Batch 1500 Loss 0.2405\n",
            "Epoch 7 Batch 1600 Loss 0.3981\n",
            "Epoch 7 Batch 1700 Loss 0.3786\n",
            "Epoch 7 Batch 1800 Loss 0.3996\n",
            "Epoch 7 Batch 1900 Loss 0.3619\n",
            "Epoch 7 Batch 2000 Loss 0.2694\n",
            "Epoch 7 Loss 0.3510\n",
            "Time taken for 1 epoch 942.6282408237457 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.2711\n",
            "Epoch 8 Batch 100 Loss 0.2680\n",
            "Epoch 8 Batch 200 Loss 0.3453\n",
            "Epoch 8 Batch 300 Loss 0.3757\n",
            "Epoch 8 Batch 400 Loss 0.3261\n",
            "Epoch 8 Batch 500 Loss 0.3612\n",
            "Epoch 8 Batch 600 Loss 0.3983\n",
            "Epoch 8 Batch 700 Loss 0.3204\n",
            "Epoch 8 Batch 800 Loss 0.4029\n",
            "Epoch 8 Batch 900 Loss 0.3170\n",
            "Epoch 8 Batch 1000 Loss 0.4321\n",
            "Epoch 8 Batch 1100 Loss 0.2950\n",
            "Epoch 8 Batch 1200 Loss 0.3320\n",
            "Epoch 8 Batch 1300 Loss 0.2956\n",
            "Epoch 8 Batch 1400 Loss 0.3440\n",
            "Epoch 8 Batch 1500 Loss 0.2410\n",
            "Epoch 8 Batch 1600 Loss 0.3711\n",
            "Epoch 8 Batch 1700 Loss 0.3472\n",
            "Epoch 8 Batch 1800 Loss 0.3961\n",
            "Epoch 8 Batch 1900 Loss 0.3669\n",
            "Epoch 8 Batch 2000 Loss 0.2811\n",
            "Epoch 8 Loss 0.3274\n",
            "Time taken for 1 epoch 951.7153527736664 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.2365\n",
            "Epoch 9 Batch 100 Loss 0.2136\n",
            "Epoch 9 Batch 200 Loss 0.3756\n",
            "Epoch 9 Batch 300 Loss 0.3785\n",
            "Epoch 9 Batch 400 Loss 0.3272\n",
            "Epoch 9 Batch 500 Loss 0.3573\n",
            "Epoch 9 Batch 600 Loss 0.3407\n",
            "Epoch 9 Batch 700 Loss 0.2974\n",
            "Epoch 9 Batch 800 Loss 0.3953\n",
            "Epoch 9 Batch 900 Loss 0.3028\n",
            "Epoch 9 Batch 1000 Loss 0.4062\n",
            "Epoch 9 Batch 1100 Loss 0.3096\n",
            "Epoch 9 Batch 1200 Loss 0.3039\n",
            "Epoch 9 Batch 1300 Loss 0.3794\n",
            "Epoch 9 Batch 1400 Loss 0.2975\n",
            "Epoch 9 Batch 1500 Loss 0.1940\n",
            "Epoch 9 Batch 1600 Loss 0.3557\n",
            "Epoch 9 Batch 1700 Loss 0.3225\n",
            "Epoch 9 Batch 1800 Loss 0.3962\n",
            "Epoch 9 Batch 1900 Loss 0.3319\n",
            "Epoch 9 Batch 2000 Loss 0.2257\n",
            "Epoch 9 Loss 0.3107\n",
            "Time taken for 1 epoch 937.2722113132477 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.2255\n",
            "Epoch 10 Batch 100 Loss 0.2218\n",
            "Epoch 10 Batch 200 Loss 0.3391\n",
            "Epoch 10 Batch 300 Loss 0.3256\n",
            "Epoch 10 Batch 400 Loss 0.2802\n",
            "Epoch 10 Batch 500 Loss 0.3440\n",
            "Epoch 10 Batch 600 Loss 0.3375\n",
            "Epoch 10 Batch 700 Loss 0.2591\n",
            "Epoch 10 Batch 800 Loss 0.3615\n",
            "Epoch 10 Batch 900 Loss 0.2655\n",
            "Epoch 10 Batch 1000 Loss 0.3189\n",
            "Epoch 10 Batch 1100 Loss 0.2740\n",
            "Epoch 10 Batch 1200 Loss 0.3009\n",
            "Epoch 10 Batch 1300 Loss 0.3578\n",
            "Epoch 10 Batch 1400 Loss 0.2694\n",
            "Epoch 10 Batch 1500 Loss 0.1819\n",
            "Epoch 10 Batch 1600 Loss 0.2874\n",
            "Epoch 10 Batch 1700 Loss 0.2846\n",
            "Epoch 10 Batch 1800 Loss 0.3586\n",
            "Epoch 10 Batch 1900 Loss 0.2773\n",
            "Epoch 10 Batch 2000 Loss 0.2407\n",
            "Epoch 10 Loss 0.2867\n",
            "Time taken for 1 epoch 938.5500741004944 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3kTPes1vbTd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "56cffc90-0233-4e33-a1ce-8d34a994c855"
      },
      "source": [
        "! cp /content/aml-nlp-notes/language-model/training_checkpoints/* /content/drive/'My Drive'/NLP-checkpoints"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: target '/content/drive/My Drive/NLP-checkpoints' is not a directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eLuStqc-cy_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "    \n",
        "    for (batch, (inp, targ)) in enumerate(dataset):\n",
        "        loss = 0\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            enc_output, enc_hidden = encoder(inp, hidden)\n",
        "            \n",
        "            dec_hidden = enc_hidden\n",
        "            \n",
        "            dec_input = tf.expand_dims([word_to_idx(word2idx, '<start>')] * BATCH_SIZE, 1)       \n",
        "            \n",
        "            # Teacher forcing - feeding the target as the next input\n",
        "            for t in range(1, targ.shape[1]):\n",
        "                # passing enc_output to the decoder\n",
        "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "                \n",
        "                loss += loss_function(targ[:, t], predictions)\n",
        "                \n",
        "                # using teacher forcing\n",
        "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "        \n",
        "        batch_loss = (loss / int(targ.shape[1]))\n",
        "        \n",
        "        total_loss += batch_loss\n",
        "        \n",
        "        variables = encoder.variables + decoder.variables\n",
        "        \n",
        "        gradients = tape.gradient(loss, variables)\n",
        "        \n",
        "        optimizer.apply_gradients(zip(gradients, variables))\n",
        "        \n",
        "        if batch % 100 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                         batch,\n",
        "                                                         batch_loss.numpy()))\n",
        "    # saving (checkpoint) the model every 2 epochs\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "    \n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                        total_loss / N_BATCH))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omQOx6iv17LH",
        "colab_type": "text"
      },
      "source": [
        "### Try training on Cornell dataset now for fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kv8aZb9V15qd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "957e2591-c719-49ac-e3e4-bedebd80323b"
      },
      "source": [
        "# Try experimenting with the size of that dataset\n",
        "\n",
        "input_tensor, target_tensor, dict_index, max_length_inp, max_length_targ = load_dataset(dataset_name = 'cornell', max_sentence_length=20)\n",
        "\n",
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 83097/83097 [00:03<00:00, 24949.75it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dh4aNHPsSZ4y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d40deeb4-3e40-4e89-9602-9a56d51fd2db"
      },
      "source": [
        "input_tensor.shape, dataset, len(dict_index[0])"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((24792, 9),\n",
              " <DatasetV1Adapter shapes: ((64, 9), (64, 9)), types: (tf.int32, tf.int32)>,\n",
              " 8739)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIhrxyi-2G0Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1207
        },
        "outputId": "a992bbaa-62a9-402b-ccd3-bb2499b384f6"
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "    \n",
        "    for (batch, (inp, targ)) in enumerate(dataset):\n",
        "        loss = 0\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            enc_output, enc_hidden = encoder(inp, hidden)\n",
        "            \n",
        "            dec_hidden = enc_hidden\n",
        "            \n",
        "            dec_input = tf.expand_dims([word_to_idx(word2idx, '<start>')] * BATCH_SIZE, 1)       \n",
        "            \n",
        "            # Teacher forcing - feeding the target as the next input\n",
        "            for t in range(1, targ.shape[1]):\n",
        "                # passing enc_output to the decoder\n",
        "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "                \n",
        "                loss += loss_function(targ[:, t], predictions)\n",
        "                \n",
        "                # using teacher forcing\n",
        "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "        \n",
        "        batch_loss = (loss / int(targ.shape[1]))\n",
        "        \n",
        "        total_loss += batch_loss\n",
        "        \n",
        "        variables = encoder.variables + decoder.variables\n",
        "        \n",
        "        gradients = tape.gradient(loss, variables)\n",
        "        \n",
        "        optimizer.apply_gradients(zip(gradients, variables))\n",
        "        \n",
        "        if batch % 100 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                         batch,\n",
        "                                                         batch_loss.numpy()))\n",
        "    # saving (checkpoint) the model every 2 epochs\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "    \n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                        total_loss / N_BATCH))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 3.9354\n",
            "Epoch 1 Batch 100 Loss 2.0047\n",
            "Epoch 1 Batch 200 Loss 1.8555\n",
            "Epoch 1 Batch 300 Loss 1.9503\n",
            "Epoch 1 Loss 0.3001\n",
            "Time taken for 1 epoch 105.25378012657166 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.8440\n",
            "Epoch 2 Batch 100 Loss 1.8041\n",
            "Epoch 2 Batch 200 Loss 1.7314\n",
            "Epoch 2 Batch 300 Loss 1.7788\n",
            "Epoch 2 Loss 0.2653\n",
            "Time taken for 1 epoch 107.89335203170776 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.7163\n",
            "Epoch 3 Batch 100 Loss 1.6674\n",
            "Epoch 3 Batch 200 Loss 1.6206\n",
            "Epoch 3 Batch 300 Loss 1.6759\n",
            "Epoch 3 Loss 0.2500\n",
            "Time taken for 1 epoch 104.43696141242981 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.6359\n",
            "Epoch 4 Batch 100 Loss 1.6132\n",
            "Epoch 4 Batch 200 Loss 1.5088\n",
            "Epoch 4 Batch 300 Loss 1.5834\n",
            "Epoch 4 Loss 0.2396\n",
            "Time taken for 1 epoch 105.70821213722229 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.5731\n",
            "Epoch 5 Batch 100 Loss 1.4957\n",
            "Epoch 5 Batch 200 Loss 1.3953\n",
            "Epoch 5 Batch 300 Loss 1.4837\n",
            "Epoch 5 Loss 0.2259\n",
            "Time taken for 1 epoch 104.60059666633606 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.5070\n",
            "Epoch 6 Batch 100 Loss 1.3818\n",
            "Epoch 6 Batch 200 Loss 1.3149\n",
            "Epoch 6 Batch 300 Loss 1.3955\n",
            "Epoch 6 Loss 0.2133\n",
            "Time taken for 1 epoch 105.12968182563782 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.4584\n",
            "Epoch 7 Batch 100 Loss 1.2942\n",
            "Epoch 7 Batch 200 Loss 1.2248\n",
            "Epoch 7 Batch 300 Loss 1.3321\n",
            "Epoch 7 Loss 0.2009\n",
            "Time taken for 1 epoch 103.52832889556885 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.3803\n",
            "Epoch 8 Batch 100 Loss 1.2604\n",
            "Epoch 8 Batch 200 Loss 1.1492\n",
            "Epoch 8 Batch 300 Loss 1.2165\n",
            "Epoch 8 Loss 0.1900\n",
            "Time taken for 1 epoch 105.43974590301514 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.3299\n",
            "Epoch 9 Batch 100 Loss 1.1585\n",
            "Epoch 9 Batch 200 Loss 1.0774\n",
            "Epoch 9 Batch 300 Loss 1.1703\n",
            "Epoch 9 Loss 0.1798\n",
            "Time taken for 1 epoch 102.65879225730896 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.2711\n",
            "Epoch 10 Batch 100 Loss 1.0514\n",
            "Epoch 10 Batch 200 Loss 0.9940\n",
            "Epoch 10 Batch 300 Loss 1.0823\n",
            "Epoch 10 Loss 0.1685\n",
            "Time taken for 1 epoch 103.47669434547424 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDdtExoV-4hV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! cp /content/aml-nlp-notes/language-model/training_checkpoints/* /content/drive/'My Drive'/NLP-checkpoints"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OG33_kV5-63l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1207
        },
        "outputId": "e2412b32-701a-4489-f552-8bf6ea72df7b"
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "    \n",
        "    for (batch, (inp, targ)) in enumerate(dataset):\n",
        "        loss = 0\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            enc_output, enc_hidden = encoder(inp, hidden)\n",
        "            \n",
        "            dec_hidden = enc_hidden\n",
        "            \n",
        "            dec_input = tf.expand_dims([word_to_idx(word2idx, '<start>')] * BATCH_SIZE, 1)       \n",
        "            \n",
        "            # Teacher forcing - feeding the target as the next input\n",
        "            for t in range(1, targ.shape[1]):\n",
        "                # passing enc_output to the decoder\n",
        "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "                \n",
        "                loss += loss_function(targ[:, t], predictions)\n",
        "                \n",
        "                # using teacher forcing\n",
        "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "        \n",
        "        batch_loss = (loss / int(targ.shape[1]))\n",
        "        \n",
        "        total_loss += batch_loss\n",
        "        \n",
        "        variables = encoder.variables + decoder.variables\n",
        "        \n",
        "        gradients = tape.gradient(loss, variables)\n",
        "        \n",
        "        optimizer.apply_gradients(zip(gradients, variables))\n",
        "        \n",
        "        if batch % 100 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                         batch,\n",
        "                                                         batch_loss.numpy()))\n",
        "    # saving (checkpoint) the model every 2 epochs\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "    \n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                        total_loss / N_BATCH))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 1.2004\n",
            "Epoch 1 Batch 100 Loss 0.9789\n",
            "Epoch 1 Batch 200 Loss 0.9537\n",
            "Epoch 1 Batch 300 Loss 1.0141\n",
            "Epoch 1 Loss 0.1584\n",
            "Time taken for 1 epoch 107.40444302558899 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.1600\n",
            "Epoch 2 Batch 100 Loss 0.8977\n",
            "Epoch 2 Batch 200 Loss 0.9267\n",
            "Epoch 2 Batch 300 Loss 0.9362\n",
            "Epoch 2 Loss 0.1474\n",
            "Time taken for 1 epoch 106.25173163414001 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.1092\n",
            "Epoch 3 Batch 100 Loss 0.8023\n",
            "Epoch 3 Batch 200 Loss 0.8478\n",
            "Epoch 3 Batch 300 Loss 0.8340\n",
            "Epoch 3 Loss 0.1367\n",
            "Time taken for 1 epoch 104.38890528678894 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.0197\n",
            "Epoch 4 Batch 100 Loss 0.7671\n",
            "Epoch 4 Batch 200 Loss 0.8009\n",
            "Epoch 4 Batch 300 Loss 0.7545\n",
            "Epoch 4 Loss 0.1268\n",
            "Time taken for 1 epoch 106.01122546195984 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.9543\n",
            "Epoch 5 Batch 100 Loss 0.7042\n",
            "Epoch 5 Batch 200 Loss 0.7270\n",
            "Epoch 5 Batch 300 Loss 0.6675\n",
            "Epoch 5 Loss 0.1168\n",
            "Time taken for 1 epoch 104.0994381904602 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.9547\n",
            "Epoch 6 Batch 100 Loss 0.6335\n",
            "Epoch 6 Batch 200 Loss 0.6408\n",
            "Epoch 6 Batch 300 Loss 0.6419\n",
            "Epoch 6 Loss 0.1100\n",
            "Time taken for 1 epoch 105.73140335083008 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.9369\n",
            "Epoch 7 Batch 100 Loss 0.5676\n",
            "Epoch 7 Batch 200 Loss 0.5702\n",
            "Epoch 7 Batch 300 Loss 0.5555\n",
            "Epoch 7 Loss 0.1005\n",
            "Time taken for 1 epoch 104.48478746414185 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.7800\n",
            "Epoch 8 Batch 100 Loss 0.5067\n",
            "Epoch 8 Batch 200 Loss 0.5106\n",
            "Epoch 8 Batch 300 Loss 0.5205\n",
            "Epoch 8 Loss 0.0903\n",
            "Time taken for 1 epoch 105.56859850883484 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.7146\n",
            "Epoch 9 Batch 100 Loss 0.4691\n",
            "Epoch 9 Batch 200 Loss 0.4506\n",
            "Epoch 9 Batch 300 Loss 0.4748\n",
            "Epoch 9 Loss 0.0819\n",
            "Time taken for 1 epoch 103.307363986969 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.6660\n",
            "Epoch 10 Batch 100 Loss 0.4686\n",
            "Epoch 10 Batch 200 Loss 0.4059\n",
            "Epoch 10 Batch 300 Loss 0.4596\n",
            "Epoch 10 Loss 0.0771\n",
            "Time taken for 1 epoch 104.82675671577454 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqEPD6PYI6P7",
        "colab_type": "text"
      },
      "source": [
        "## Twitter data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFw9tueJI5qS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e6e223bc-b958-4a42-df40-1d40d4068863"
      },
      "source": [
        "! git clone https://github.com/Marsan-Ma/chat_corpus"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'chat_corpus'...\n",
            "remote: Enumerating objects: 17, done.\u001b[K\n",
            "remote: Total 17 (delta 0), reused 0 (delta 0), pack-reused 17\u001b[K\n",
            "Unpacking objects: 100% (17/17), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVvt57INJauf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! mv chat_corpus/ data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2CiImfEJ-eU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! gzip -d data/chat_corpus/twitter_en.txt.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHEkf5_iKGbC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "bb5ef7ea-6476-4f7d-f5c9-0d5f60b2a625"
      },
      "source": [
        "! ls data/chat_corpus/"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lyrics_zh.txt.gz\t   README.md\t\t\t twitter_en.txt\n",
            "movie_subtitles_en.txt.gz  twitter_en_big.txt.gz.partaa\n",
            "open_subtitles.txt.gz\t   twitter_en_big.txt.gz.partab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0AWOBZSKNqX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Try experimenting with the size of that dataset\n",
        "\n",
        "input_tensor, target_tensor, dict_index, max_length_inp, max_length_targ = load_dataset(dataset_name = 'twitter', max_sentence_length=20)\n",
        "\n",
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-HrwWlYRTSc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "3c9ef8f7-d504-4c99-ed6d-792cfd38119d"
      },
      "source": [
        "input_tensor.shape, dataset, len(dict_index[0])"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((377265, 41),\n",
              " <DatasetV1Adapter shapes: ((64, 41), (64, 39)), types: (tf.int32, tf.int32)>,\n",
              " 15002)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8H-1S7GQt_W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "08bae83c-e1a1-43f7-8dc9-6a4e2e2a04a5"
      },
      "source": [
        "EPOCHS = 15\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "    \n",
        "    for (batch, (inp, targ)) in enumerate(dataset):\n",
        "        loss = 0\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            enc_output, enc_hidden = encoder(inp, hidden)\n",
        "            \n",
        "            dec_hidden = enc_hidden\n",
        "            \n",
        "            dec_input = tf.expand_dims([word_to_idx(word2idx, '<start>')] * BATCH_SIZE, 1)       \n",
        "            \n",
        "            # Teacher forcing - feeding the target as the next input\n",
        "            for t in range(1, targ.shape[1]):\n",
        "                # passing enc_output to the decoder\n",
        "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "                \n",
        "                loss += loss_function(targ[:, t], predictions)\n",
        "                \n",
        "                # using teacher forcing\n",
        "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "        \n",
        "        batch_loss = (loss / int(targ.shape[1]))\n",
        "        \n",
        "        total_loss += batch_loss\n",
        "        \n",
        "        variables = encoder.variables + decoder.variables\n",
        "        \n",
        "        gradients = tape.gradient(loss, variables)\n",
        "        \n",
        "        optimizer.apply_gradients(zip(gradients, variables))\n",
        "        \n",
        "        if batch % 100 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                         batch,\n",
        "                                                         batch_loss.numpy()))\n",
        "    # saving (checkpoint) the model every 2 epochs\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "    \n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                        total_loss / N_BATCH))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 3.4326\n",
            "Epoch 1 Batch 100 Loss 2.2943\n",
            "Epoch 1 Batch 200 Loss 2.1855\n",
            "Epoch 1 Batch 300 Loss 2.0957\n",
            "Epoch 1 Batch 400 Loss 1.8933\n",
            "Epoch 1 Batch 500 Loss 1.8357\n",
            "Epoch 1 Batch 600 Loss 1.9868\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRd40gqNbBeu",
        "colab_type": "text"
      },
      "source": [
        "## Prediction \n",
        "* The evaluate function is similar to the training loop, except we don't use *teacher forcing* here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n",
        "* Stop predicting when the model predicts the *end token*.\n",
        "* And store the *attention weights for every time step*.\n",
        "\n",
        "Note: The encoder output is calculated only once for one input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8B3xajV6bBey",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(sentence, encoder, decoder, dictionary_index,  max_length):\n",
        "    word2idx = dictionary_index[0]\n",
        "    idx2word = dictionary_index[1]\n",
        "    attention_plot = np.zeros((max_length, max_length))\n",
        "    \n",
        "    sentence = preprocess_sentence(sentence)\n",
        "\n",
        "    inputs = [word_to_idx(word2idx, i) for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length, padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    \n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([word_to_idx(word2idx, '<start>')word2idx['<start>']], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
        "        \n",
        "        # storing the attention weights to plot later on\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += idx2word[predicted_id] + ' '\n",
        "\n",
        "        if idx2word[predicted_id] == '<end>':\n",
        "            return result, sentence, attention_plot\n",
        "        \n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence, attention_plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tL0a5uQ0bBe0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attention, cmap='viridis')\n",
        "    \n",
        "    fontdict = {'fontsize': 14}\n",
        "    \n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bFl4ihwbBe8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def answer(sentence, encoder, decoder, dictionary_index, max_length):\n",
        "    result, sentence, attention_plot = evaluate(sentence, encoder, decoder, dictionary_index, max_length)\n",
        "        \n",
        "    print('Input: {}'.format(sentence))\n",
        "    print('Predicted translation: {}'.format(result))\n",
        "    \n",
        "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FESq_uq1bBe_",
        "colab_type": "text"
      },
      "source": [
        "## Testing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66aL1DKVbBe_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbUSSL-bMNKx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f0f5de5a-0920-406d-c2b9-009d71e70a38"
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt-5'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7n3KWeq9fx2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3c240a5b-e4cc-46e5-a243-be9d0d8009cb"
      },
      "source": [
        "preprocess_sentence(qs[1])"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> how are you doing <end>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0njLqHRv8-42",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "qs = ['Hi', 'How are you doing?', \"What is your name?\",  \"What's your hobby?\", \"What is AI?\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2kRgGfSbBfC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "outputId": "4af21391-d03c-4b12-d8b0-8ee83abb0a26"
      },
      "source": [
        "answer(qs[4], encoder, decoder, dict_index, max_length_targ)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-0686fd9d36ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0manswer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length_targ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-33-3cb71f05c261>\u001b[0m in \u001b[0;36manswer\u001b[0;34m(sentence, encoder, decoder, dictionary_index, max_length)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Predicted translation: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-2bd6654a6883>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(sentence, encoder, decoder, dictionary_index, max_length)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-2bd6654a6883>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'ai'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1RSYoYibBfG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "outputId": "813451ac-5567-4202-bd7c-a709c8768392"
      },
      "source": [
        "answer('How are you doing', encoder, decoder, dict_index, max_length_targ)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> how are you doing <end>\n",
            "Predicted translation: i dont know <end> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAngAAAIiCAYAAABbgrWrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njMADABjM/wcwMOT/6F08mwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exPDTfQxbBfJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "outputId": "a4a40ff4-6b65-469d-ef48-34d2a58c9df2"
      },
      "source": [
        "answer('Do you know anything', encoder, decoder, dict_index, max_length_targ)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> do you know anything <end>\n",
            "Predicted translation: no <end> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAngAAAFtCAYAAACUZBePAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WatTBtGsbBfN",
        "colab_type": "text"
      },
      "source": [
        "## Scratchpad"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cUrxXiObBfR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_length_inp, max_length_targ "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBMaqnXEbBfa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_tensor_train[0], 1 - np.equal(target_tensor_train[0], 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J36POnBsbBff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_tensor_train[0], target_tensor_train[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7g9WRA7bBfo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "q_index = create_index([q for q, a in data])\n",
        "q_word2idx = q_index[0]\n",
        "len(q_word2idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gY2Hl6wSbBfu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[[q_word2idx[w] for w in qs.split(' ')] for qs, a in data]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "4XEt3oFJbBfx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data[:100]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AueKpR5bBf1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "create_index([a for a,b in data[:100]])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}